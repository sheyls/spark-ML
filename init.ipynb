{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JAVA_TOOL_OPTIONS'] = '-Djava.security.manager=allow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T17:51:45.654351Z",
     "start_time": "2025-01-10T17:51:45.646774Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<<<<<<< local <modified: >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Picked up JAVA_TOOL_OPTIONS: -Djava.security.manager=allow\n",
      "Picked up JAVA_TOOL_OPTIONS: -Djava.security.manager=allow\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/10 19:14:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>>>>>> remote <removed>\n"
     ]
    }
   ],
   "source": [
    "# =================== 1. Setup Spark and Import Libraries ===================\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import RandomForestRegressor, DecisionTreeRegressor, LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import RandomForestRegressor, DecisionTreeRegressor, LinearRegression, GBTRegressor, GeneralizedLinearRegression, IsotonicRegression\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import argparse\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.tuning import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "import traceback\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "from pyspark import keyword_only\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml import Pipeline, Transformer, Estimator, PipelineModel\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.sql import functions as F\n",
    "import json\n",
    "from pyspark.sql.types import IntegerType, DoubleType, FloatType, StructField, Row\n",
    "from pyspark.sql.functions import col, sum\n",
    "from math import pi, cos, sin\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.context import SparkContext as sc\n",
    "import os\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"MachineLearningProject\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = './data/2008.csv'\n",
    "plane_data_path = './data/plane-data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(spark, df_path, planes_data_path) -> DataFrame:\n",
    "    # Read csv\n",
    "    df = spark.read.csv(\n",
    "        df_path,\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    forbidden_cols = [\n",
    "        \"ArrTime\",\n",
    "        \"ActualElapsedTime\",\n",
    "        \"AirTime\",\n",
    "        \"TaxiIn\",\n",
    "        \"Diverted\",\n",
    "        \"CarrierDelay\",\n",
    "        \"WeatherDelay\",\n",
    "        \"NASDelay\",\n",
    "        \"SecurityDelay\",\n",
    "        \"LateAircraftDelay\"\n",
    "    ]\n",
    "    df = df.drop(*forbidden_cols)\n",
    "\n",
    "    df_planes = spark.read.csv(\n",
    "        planes_data_path,\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    df_planes = df_planes.withColumnRenamed(\"tailnum\", \"TailNum\")\n",
    "    df_planes = df_planes.withColumnRenamed(\"year\", \"PlaneIssueYear\")\n",
    "    df_planes = df_planes.withColumnRenamed(\"engine_type\", \"EngineType\")\n",
    "    df_planes = df_planes.withColumnRenamed(\"aircraft_type\", \"AircraftType\")\n",
    "    df_planes = df_planes.withColumnRenamed(\"model\", \"Model\")\n",
    "    df_planes = df_planes.withColumnRenamed(\"manufacturer\", \"Manufacturer\")\n",
    "\n",
    "    data = df.join(df_planes, on=\"TailNum\", how=\"inner\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_data(df):\n",
    "    quant_time_features = [\n",
    "        'DepTime',\n",
    "        'CRSDepTime',\n",
    "        'CRSArrTime'\n",
    "    ]\n",
    "\n",
    "    quantitative_features = [\n",
    "            'CRSElapsedTime',\n",
    "            'DepDelay',\n",
    "            'Distance',\n",
    "            'TaxiOut',\n",
    "            'PlaneIssueYear'\n",
    "        ]\n",
    "\n",
    "    target_column = \"ArrDelay\"\n",
    "\n",
    "    for column in quantitative_features + [target_column]:\n",
    "        df = df.withColumn(column, col(column).cast(IntegerType()))\n",
    "    df = df.dropna(subset=[target_column])\n",
    "    null_count = df.filter(col(target_column).isNull()).count()\n",
    "\n",
    "    for column in quant_time_features:  # They are strings hhmm\n",
    "        df = df.withColumn(\n",
    "            column + \"_minutes\",\n",
    "            (F.col(column).substr(1, 2).cast(\"int\") * 60 + F.col(column).substr(3, 2).cast(\"int\"))\n",
    "        )\n",
    "        quantitative_features.append(column + \"_minutes\")\n",
    "    df = df.drop(*quant_time_features)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- CRSElapsedTime: integer (nullable = true)\n",
      " |-- ArrDelay: integer (nullable = true)\n",
      " |-- DepDelay: integer (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- TaxiOut: integer (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- Manufacturer: string (nullable = true)\n",
      " |-- issue_date: string (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- AircraftType: string (nullable = true)\n",
      " |-- EngineType: string (nullable = true)\n",
      " |-- PlaneIssueYear: integer (nullable = true)\n",
      " |-- DepTime_minutes: integer (nullable = true)\n",
      " |-- CRSDepTime_minutes: integer (nullable = true)\n",
      " |-- CRSArrTime_minutes: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = load_csv(spark, csv_path, plane_data_path)\n",
    "df = organize_data(df)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\n",
    "    'Month',\n",
    "    'DayofMonth',\n",
    "    'DayOfWeek',\n",
    "    'Year',\n",
    "    'PlaneIssueYear',\n",
    "    'DepTime_minutes',\n",
    "    'CRSDepTime_minutes',\n",
    "    'CRSArrTime_minutes',\n",
    "    'CRSElapsedTime',\n",
    "    'DepDelay',\n",
    "    'Distance',\n",
    "    'TaxiOut'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    "    'UniqueCarrier',\n",
    "    'FlightNum',\n",
    "    'TailNum',\n",
    "    'Origin',\n",
    "    'Dest',\n",
    "    'Cancelled',\n",
    "    'CancellationCode',\n",
    "    'EngineType',\n",
    "    'AircraftType',\n",
    "    'Manufacturer',\n",
    "    'Model',\n",
    "    \"issue_date\", \"status\",\n",
    "    \"type\",\n",
    "    \"ArrDelay\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 2235032\n",
      "Number of columns: 27\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rows: {df.count()}\")\n",
    "print(f\"Number of columns: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_values(data, features_list):\n",
    "    # Calculate null values for each column in features_list\n",
    "    null_data = data.select([\n",
    "        count(when(col(c).isNull() | isnan(col(c)), c)).alias(c) for c in features_list\n",
    "    ])\n",
    "    \n",
    "    # Show the results\n",
    "    null_data.show()\n",
    "    \n",
    "    # Return the DataFrame with null counts\n",
    "    return null_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_null_percentages(df, null_counts, numeric):# Convert to Pandas and compute percentage\n",
    "    if numeric == True:\n",
    "        type = 'Numerical'\n",
    "        color = 'skyblue'\n",
    "    else:\n",
    "        type = 'Categorical'\n",
    "        color = 'lightcoral'\n",
    "    total_rows = df.count()\n",
    "    null_counts_pandas = null_counts.toPandas().T  # Transpose for easier handling\n",
    "    null_counts_pandas.columns = [\"NullCount\"]\n",
    "    null_counts_pandas[\"Percentage\"] = (null_counts_pandas[\"NullCount\"] / total_rows) * 100\n",
    "    null_counts_pandas = null_counts_pandas.sort_values(\"Percentage\", ascending=False)\n",
    "\n",
    "    # Plot the bar chart\n",
    "    null_counts_pandas[\"Percentage\"].plot(kind=\"barh\", color=color)  # Use `barh` for horizontal bars\n",
    "    plt.xlabel(\"Percentage of Null Values (%)\")\n",
    "    plt.ylabel(f\"{type} Features\")\n",
    "    plt.title(f\"Percentage of Null Values by {type} Features\")\n",
    "    plt.savefig(f\"output/{type.lower()}/img/null_values_percentage_{type.lower()}.png\", dpi=300, bbox_inches=\"tight\")  # Save as PNG\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:=====>                                                   (1 + 9) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------+----+--------------+---------------+------------------+------------------+--------------+--------+--------+-------+\n",
      "|Month|DayofMonth|DayOfWeek|Year|PlaneIssueYear|DepTime_minutes|CRSDepTime_minutes|CRSArrTime_minutes|CRSElapsedTime|DepDelay|Distance|TaxiOut|\n",
      "+-----+----------+---------+----+--------------+---------------+------------------+------------------+--------------+--------+--------+-------+\n",
      "|    0|         0|        0|   0|        176935|           6950|              3077|             19365|             0|       0|       0|      0|\n",
      "+-----+----------+---------+----+--------------+---------------+------------------+------------------+--------------+--------+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "null_counts_numeric = null_values(df, numeric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "plot_null_percentages(df, null_counts_numeric, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:============================>                            (5 + 5) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-------+------+----+---------+----------------+----------+------------+------------+------+----------+------+------+--------+\n",
      "|UniqueCarrier|FlightNum|TailNum|Origin|Dest|Cancelled|CancellationCode|EngineType|AircraftType|Manufacturer| Model|issue_date|status|  type|ArrDelay|\n",
      "+-------------+---------+-------+------+----+---------+----------------+----------+------------+------------+------+----------+------+------+--------+\n",
      "|            0|        0|      0|     0|   0|        0|         2235032|    107854|      107854|      107854|107854|    107854|107854|107854|       0|\n",
      "+-------------+---------+-------+------+----+---------+----------------+----------+------------+------------+------+----------+------+------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "null_counts_categorical = null_values(df, categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/10 19:15:11 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "plot_null_percentages(df, null_counts_categorical, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Statistics Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics_summary(data):\n",
    "    summary_df = data.select(numeric_features).summary().toPandas()\n",
    "    summary_df.set_index(\"summary\", inplace=True)\n",
    "    summary_numeric = summary_df.apply(pd.to_numeric, errors='coerce')\n",
    "    summary_numeric = summary_numeric.T\n",
    "    return summary_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/10 19:15:13 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>summary</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>stddev</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Month</th>\n",
       "      <td>2235032.0</td>\n",
       "      <td>2.511060</td>\n",
       "      <td>1.123274e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DayofMonth</th>\n",
       "      <td>2235032.0</td>\n",
       "      <td>15.695071</td>\n",
       "      <td>8.745910e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DayOfWeek</th>\n",
       "      <td>2235032.0</td>\n",
       "      <td>3.915981</td>\n",
       "      <td>1.982679e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>2235032.0</td>\n",
       "      <td>2008.000000</td>\n",
       "      <td>1.908099e-13</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>2008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PlaneIssueYear</th>\n",
       "      <td>2058097.0</td>\n",
       "      <td>1995.048672</td>\n",
       "      <td>6.806124e+01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1992.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>2008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DepTime_minutes</th>\n",
       "      <td>2228082.0</td>\n",
       "      <td>1947.945860</td>\n",
       "      <td>1.662226e+03</td>\n",
       "      <td>600.0</td>\n",
       "      <td>839.0</td>\n",
       "      <td>1081.0</td>\n",
       "      <td>3604.0</td>\n",
       "      <td>5709.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRSDepTime_minutes</th>\n",
       "      <td>2231955.0</td>\n",
       "      <td>1966.182544</td>\n",
       "      <td>1.685970e+03</td>\n",
       "      <td>600.0</td>\n",
       "      <td>830.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>3660.0</td>\n",
       "      <td>5709.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRSArrTime_minutes</th>\n",
       "      <td>2215667.0</td>\n",
       "      <td>1614.865577</td>\n",
       "      <td>1.467061e+03</td>\n",
       "      <td>600.0</td>\n",
       "      <td>835.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1310.0</td>\n",
       "      <td>5709.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRSElapsedTime</th>\n",
       "      <td>2235032.0</td>\n",
       "      <td>130.778137</td>\n",
       "      <td>7.055441e+01</td>\n",
       "      <td>-21.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>660.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DepDelay</th>\n",
       "      <td>2235032.0</td>\n",
       "      <td>11.369590</td>\n",
       "      <td>3.627695e+01</td>\n",
       "      <td>-92.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2467.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Distance</th>\n",
       "      <td>2235032.0</td>\n",
       "      <td>740.248358</td>\n",
       "      <td>5.652384e+02</td>\n",
       "      <td>24.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>590.0</td>\n",
       "      <td>983.0</td>\n",
       "      <td>4962.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TaxiOut</th>\n",
       "      <td>2235032.0</td>\n",
       "      <td>16.579896</td>\n",
       "      <td>1.100051e+01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>383.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "summary                 count         mean        stddev     min     25%  \\\n",
       "Month               2235032.0     2.511060  1.123274e+00     1.0     1.0   \n",
       "DayofMonth          2235032.0    15.695071  8.745910e+00     1.0     8.0   \n",
       "DayOfWeek           2235032.0     3.915981  1.982679e+00     1.0     2.0   \n",
       "Year                2235032.0  2008.000000  1.908099e-13  2008.0  2008.0   \n",
       "PlaneIssueYear      2058097.0  1995.048672  6.806124e+01     0.0  1992.0   \n",
       "DepTime_minutes     2228082.0  1947.945860  1.662226e+03   600.0   839.0   \n",
       "CRSDepTime_minutes  2231955.0  1966.182544  1.685970e+03   600.0   830.0   \n",
       "CRSArrTime_minutes  2215667.0  1614.865577  1.467061e+03   600.0   835.0   \n",
       "CRSElapsedTime      2235032.0   130.778137  7.055441e+01   -21.0    80.0   \n",
       "DepDelay            2235032.0    11.369590  3.627695e+01   -92.0    -4.0   \n",
       "Distance            2235032.0   740.248358  5.652384e+02    24.0   329.0   \n",
       "TaxiOut             2235032.0    16.579896  1.100051e+01     0.0    10.0   \n",
       "\n",
       "summary                50%     75%     max  \n",
       "Month                  3.0     4.0     4.0  \n",
       "DayofMonth            16.0    23.0    31.0  \n",
       "DayOfWeek              4.0     6.0     7.0  \n",
       "Year                2008.0  2008.0  2008.0  \n",
       "PlaneIssueYear      2000.0  2003.0  2008.0  \n",
       "DepTime_minutes     1081.0  3604.0  5709.0  \n",
       "CRSDepTime_minutes  1070.0  3660.0  5709.0  \n",
       "CRSArrTime_minutes  1070.0  1310.0  5709.0  \n",
       "CRSElapsedTime       113.0   162.0   660.0  \n",
       "DepDelay               0.0    11.0  2467.0  \n",
       "Distance             590.0   983.0  4962.0  \n",
       "TaxiOut               14.0    19.0   383.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = statistics_summary(df)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Features Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_distributions(data, features_list, is_numeric_features=True):\n",
    "    if is_numeric_features == True:\n",
    "        type = 'Numerical'\n",
    "        numerical_df = data.select(features_list).toPandas()\n",
    "\n",
    "        num_features = len(features_list)\n",
    "        cols = 2  # Number of columns in the grid\n",
    "        rows = (num_features // cols) + (num_features % cols > 0)  # Calculate rows needed\n",
    "\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(15, rows * 5))  # Adjust the figure size\n",
    "        axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
    "\n",
    "        # Plot each feature's distribution\n",
    "        for i, col in enumerate(features_list):\n",
    "            sns.histplot(numerical_df[col], bins=30, ax=axes[i])  # Use the subplot's axis\n",
    "            axes[i].set_title(f\"Distribution of {col}\")\n",
    "            axes[i].set_xlabel(col)\n",
    "            axes[i].set_ylabel(\"Frequency\")\n",
    "\n",
    "            if col == \"DepDelay\":  # Modify based on feature name\n",
    "                axes[i].set_xlim(0, 500)  # Set x-axis range (e.g., 0 to 500)\n",
    "            elif col == \"TaxiOut\":\n",
    "                axes[i].set_xlim(0, 150)\n",
    "\n",
    "        # Remove any unused subplots\n",
    "        for i in range(len(features_list), len(axes)):\n",
    "            fig.delaxes(axes[i])\n",
    "\n",
    "        plt.tight_layout()  # Adjust layout to avoid overlap\n",
    "        plt.savefig(f\"output/{type.lower()}/img/features_distribution_{type.lower()}.png\", dpi=300, bbox_inches=\"tight\")  # Save as PNG\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        type = 'Categorical'\n",
    "        num_features = len(features_list)\n",
    "        cols = 2  # Number of columns in the grid\n",
    "        rows = (num_features // cols) + (num_features % cols > 0)  # Calculate rows needed\n",
    "\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(15, rows * 5))  # Adjust figure size\n",
    "        axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
    "\n",
    "        # Plot each feature's distribution\n",
    "        for i, col in enumerate(features_list):\n",
    "            # Group by column and count occurrences\n",
    "            # Limit to top 15 categories\n",
    "            if col == \"ArrDelay\":  # Special case for \"ArrDelay\"\n",
    "                # Group by column and count occurrences for all data\n",
    "                category_counts = data.groupBy(col).count().orderBy(\"count\", ascending=False)\n",
    "            else:\n",
    "                # Limit to top 15 categories for other features\n",
    "                top_n = 15\n",
    "                category_counts = data.groupBy(col).count().orderBy(\"count\", ascending=False).limit(top_n)\n",
    "\n",
    "            category_df = category_counts.toPandas()\n",
    "\n",
    "            # Plot using the subplot axis\n",
    "            sns.barplot(data=category_df, x=col, y=\"count\", ax=axes[i])\n",
    "            axes[i].set_title(f\"Distribution of {col}\")\n",
    "            axes[i].set_xlabel(col)\n",
    "            axes[i].set_ylabel(\"Count\")\n",
    "            axes[i].tick_params(axis=\"x\", rotation=90)  # Rotate x-axis labels\n",
    "\n",
    "            if col == \"ArrDelay\":\n",
    "                axes[i].set_xlim(50, 120)\n",
    "                axes[i].tick_params(axis=\"x\", labelsize=8)\n",
    "\n",
    "        # Remove any unused subplots\n",
    "        for i in range(len(features_list), len(axes)):\n",
    "            fig.delaxes(axes[i])\n",
    "\n",
    "        plt.tight_layout()  # Adjust layout to avoid overlap\n",
    "        plt.savefig(f\"output/{type.lower()}/img/features_distribution_{type.lower()}.png\", dpi=300, bbox_inches=\"tight\")  # Save as PNG\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "features_distributions(df, ['DayofMonth','DayOfWeek','CRSDepTime_minutes','CRSArrTime_minutes','CRSElapsedTime','DepDelay','Distance','TaxiOut'], is_numeric_features=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "features_distributions(df, ['UniqueCarrier','Origin','Dest','EngineType','AircraftType','Manufacturer','Model','ArrDelay'], is_numeric_features=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Features Proportions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportions(data, features_list, is_numeric_features = True):\n",
    "    if is_numeric_features == True:\n",
    "        type = 'numerical'\n",
    "    else:\n",
    "        type = 'categorical'\n",
    "\n",
    "    total_count = data.count()\n",
    "    for feature in features_list:\n",
    "        feature_counts = df.groupBy(feature).count()\n",
    "        # Calculate proportions\n",
    "        feature_proportions = feature_counts.withColumn(\n",
    "            \"Proportion\", round((col(\"count\") / total_count)*100,2)\n",
    "        )\n",
    "        if not os.path.exists(f'output/{type}'):\n",
    "            os.makedirs(f'output/{type}')\n",
    "        feature_proportions.write.csv(f\"output/{type}/{feature}_proportions.csv\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "proportions(df, categorical_features, is_numeric_features=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "proportions(df, numeric_features, is_numeric_features=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Average ArrDelay by categorical features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_ArrDelay(data, features_list):\n",
    "    # Number of features\n",
    "    num_features = len(features_list)\n",
    "    cols = 2  # Number of columns in the grid\n",
    "    rows = (num_features // cols) + (num_features % cols > 0)  # Calculate rows needed\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, rows * 6))  # Adjust figure size\n",
    "    axes = axes.flatten()  # Flatten axes for easy iteration\n",
    "\n",
    "    # Iterate over each categorical feature\n",
    "    for i, col_name in enumerate(features_list):  # Use col_name for clarity\n",
    "        if col_name in [\"Origin\", \"Dest\", \"Model\"]:  # Special case for \"Origin\" and \"Dest\"\n",
    "            type = 'categorical'\n",
    "            top_n = 20\n",
    "            category_counts = data.groupBy(col_name).count().orderBy(\"count\", ascending=False).limit(top_n)\n",
    "            # Filter data for the top 20 categories\n",
    "            top_categories = [row[col_name] for row in category_counts.collect()]\n",
    "            data_filtered = data.filter(col(col_name).isin(top_categories))  # Correct usage of col()\n",
    "        else:\n",
    "            type = 'numerical'\n",
    "            data_filtered = data\n",
    "\n",
    "        # Group data and calculate average arrival delay\n",
    "        grouped_df = data_filtered.groupBy(col_name).agg({\"ArrDelay\": \"mean\"})\n",
    "\n",
    "        # Convert to Pandas\n",
    "        grouped_pandas = grouped_df.toPandas()\n",
    "\n",
    "        # Plot bar chart in the subplot\n",
    "        sns.barplot(data=grouped_pandas, x=col_name, y=\"avg(ArrDelay)\", ax=axes[i])\n",
    "        axes[i].set_title(f\"Average Arrival Delay by {col_name}\", fontsize=12)\n",
    "        axes[i].set_xlabel(col_name)\n",
    "        axes[i].set_ylabel(\"Avg. Arrival Delay\")\n",
    "        axes[i].tick_params(axis=\"x\", rotation=90)\n",
    "\n",
    "        if col_name in [\"DepTime_minutes\", \"DepDelay\"]:\n",
    "            xticks = axes[i].get_xticks()\n",
    "            axes[i].set_xticks([xticks[0], xticks[-1]])\n",
    "            axes[i].set_xticklabels([grouped_pandas[col_name].iloc[0], grouped_pandas[col_name].iloc[-1]])  # Set corresponding labels\n",
    "\n",
    "\n",
    "    # Remove any unused subplots\n",
    "    for i in range(len(features_list), len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"output/{type.lower()}/img/avg_ArrDelay_{type.lower()}.png\", dpi=300, bbox_inches=\"tight\")  # Save as PNG\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "avg_ArrDelay(df, ['Origin','Dest','EngineType','AircraftType','Manufacturer','Model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "avg_ArrDelay(df, ['Month','DayofMonth','DayOfWeek','PlaneIssueYear','DepTime_minutes','DepDelay'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Correlation Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_matrix(data, features_list):\n",
    "    data = data.fillna(0, subset=features_list)\n",
    "    vector_col = \"features_corr\"\n",
    "\n",
    "    vector_assembler = VectorAssembler(inputCols=features_list, outputCol=vector_col)\n",
    "    df_vector = vector_assembler.transform(data)\n",
    "\n",
    "    # Compute Correlation Matrix\n",
    "    correlation_matrix = Correlation.corr(df_vector, vector_col).head()[0]  # Get the DenseMatrix\n",
    "    correlation_array = correlation_matrix.toArray()\n",
    "    correlation_df = pd.DataFrame(correlation_array, index=features_list, columns=features_list)\n",
    "    sns.heatmap(\n",
    "        correlation_df,\n",
    "        annot=True,              # Show the correlation values\n",
    "        fmt=\".1f\",               # Format to two decimal places\n",
    "        cmap=\"coolwarm\",         # Color map\n",
    "        annot_kws={\"size\": 8}    # Reduce annotation font size\n",
    "    )\n",
    "    plt.title(\"Correlation Matrix Heatmap\")\n",
    "    plt.savefig(f\"output/numerical/img/correlation_matrix.png\", dpi=300, bbox_inches=\"tight\")  # Save as PNG\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/10 19:16:24 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/01/10 19:16:24 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "25/01/10 19:16:25 WARN PearsonCorrelation: Pearson correlation matrix contains NaN values.\n"
     ]
    }
   ],
   "source": [
    "corr_matrix(df, numeric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descomprimiendo archivos...\n",
      "Advertencia: 1991.csv no encontrado en 1991.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 1991.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 1991.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 1991.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 1991.\n",
      "Advertencia: 2004.csv no encontrado en 2004.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 2004.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 2004.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 2004.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 2004.\n",
      "Advertencia: 1988.csv no encontrado en 1988.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 1988.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 1988.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 1988.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 1988.\n",
      "Advertencia: 1998.csv no encontrado en 1998.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 1998.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 1998.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 1998.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 1998.\n",
      "Advertencia: 1999.csv no encontrado en 1999.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 1999.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 1999.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 1999.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 1999.\n",
      "Advertencia: 1989.csv no encontrado en 1989.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 1989.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 1989.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 1989.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 1989.\n",
      "Advertencia: 1990.csv no encontrado en 1990.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 1990.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 1990.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 1990.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 1990.\n",
      "Advertencia: 2005.csv no encontrado en 2005.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 2005.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 2005.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 2005.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 2005.\n",
      "Advertencia: 2007.csv no encontrado en 2007.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 2007.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 2007.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 2007.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 2007.\n",
      "Advertencia: 1992.csv no encontrado en 1992.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 1992.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 1992.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 1992.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 1992.\n",
      "Advertencia: 2006.csv no encontrado en 2006.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 2006.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 2006.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 2006.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 2006.\n",
      "Advertencia: 1993.csv no encontrado en 1993.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 1993.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 1993.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 1993.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 1993.\n",
      "Advertencia: 1996.csv no encontrado en 1996.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 1996.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 1996.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 1996.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 1996.\n",
      "Advertencia: 2003.csv no encontrado en 2003.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 2003.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 2003.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 2003.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 2003.\n",
      "Advertencia: 1987.csv no encontrado en 1987.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 1987.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 1987.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 1987.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 1987.\n",
      "Advertencia: 1997.csv no encontrado en 1997.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 1997.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 1997.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 1997.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 1997.\n",
      "Advertencia: 2002.csv no encontrado en 2002.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 2002.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 2002.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 2002.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 2002.\n",
      "Advertencia: 2000.csv no encontrado en 2000.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 2000.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 2000.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 2000.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 2000.\n",
      "Advertencia: 1995.csv no encontrado en 1995.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 1995.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 1995.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 1995.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 1995.\n",
      "Advertencia: 2008.csv no encontrado en 2008.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 2008.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 2008.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 2008.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 2008.\n",
      "Advertencia: 2001.csv no encontrado en 2001.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 2001.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 2001.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 2001.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 2001.\n",
      "Advertencia: 1994.csv no encontrado en 1994.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 1994.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 1994.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 1994.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 1994.\n",
      "Procesando todos los datos anuales...\n",
      "Procesando datos del año: 1988\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/Users/sheyls/study/big_data/spark-ML/data/new/1988/1988.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 100\u001b[0m\n\u001b[1;32m     97\u001b[0m combined_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m year_folder \u001b[38;5;129;01min\u001b[39;00m year_folders:\n\u001b[0;32m--> 100\u001b[0m     yearly_data \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_yearly_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m combined_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m         combined_data \u001b[38;5;241m=\u001b[39m yearly_data\n",
      "Cell \u001b[0;32mIn[19], line 69\u001b[0m, in \u001b[0;36mprocess_yearly_data\u001b[0;34m(year_folder)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcesando datos del año: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Leer los archivos\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m flights \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43myear\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m plane_data \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(year_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplane-data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m), header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     71\u001b[0m carriers \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(year_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcarriers.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m), header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/Users/sheyls/study/big_data/spark-ML/data/new/1988/1988.csv."
     ]
    }
   ],
   "source": [
    "# =================== 1. Data Reading ===================\n",
    "def load_data(spark, input_path, mode):\n",
    "    \"\"\"\n",
    "    Load the input dataset, drop forbidden columns, and validate its structure.\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): The active Spark session.\n",
    "        input_path (str): Path to the input CSV file.\n",
    "        mode (str): Mode of operation (\"train\" or \"predict\").\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Processed Spark DataFrame.\n",
    "    \"\"\"\n",
    "    forbidden_columns = [\n",
    "        \"ArrTime\", \"ActualElapsedTime\", \"AirTime\", \"TaxiIn\", \"Diverted\",\n",
    "        \"CarrierDelay\", \"WeatherDelay\", \"NASDelay\", \"SecurityDelay\", \"LateAircraftDelay\"\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # Read the dataset\n",
    "        data = spark.read.csv(input_path, header=True, inferSchema=True)\n",
    "\n",
    "        # Drop forbidden columns\n",
    "        data = data.drop(*forbidden_columns)\n",
    "\n",
    "        # Check if the dataset is empty\n",
    "        if data.count() == 0:\n",
    "            raise ValueError(\"The dataset is empty.\")\n",
    "\n",
    "        # Validate the presence of the target variable for training\n",
    "        if mode == \"train\" and \"ArrDelay\" not in data.columns:\n",
    "            raise ValueError(\"The target variable 'ArrDelay' is missing.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the dataset: {e}\")\n",
    "        spark.stop()\n",
    "        raise\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== 2. Exploratory Data Analysis (EDA) ===================\n",
    "def eda(data):\n",
    "    \"\"\"\n",
    "    Perform exploratory data analysis on the dataset, including univariate and multivariate analysis.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): Spark DataFrame to analyze.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with a feature vector column added for further processing.\n",
    "    \"\"\"\n",
    "    # Univariate analysis: Display statistical summary\n",
    "    print(\"Statistical Summary:\")\n",
    "    data.describe().show()\n",
    "\n",
    "    # Multivariate analysis: Correlations and patterns\n",
    "    # Select numeric columns for correlation analysis\n",
    "    numeric_cols = [col for col, dtype in data.dtypes if dtype in ('int', 'double')]\n",
    "\n",
    "    if numeric_cols:\n",
    "        # Assemble numeric columns into a single feature vector\n",
    "        vector_col = \"features_vector\"\n",
    "        assembler = VectorAssembler(inputCols=numeric_cols, outputCol=vector_col)\n",
    "        data = assembler.transform(data)\n",
    "\n",
    "        # Display correlation matrix for the numeric features\n",
    "        from pyspark.ml.stat import Correlation\n",
    "        try:\n",
    "            correlation_matrix = Correlation.corr(data, vector_col).head()[0]\n",
    "            print(f\"Correlation matrix:\\n{correlation_matrix}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating correlations: {e}\")\n",
    "    else:\n",
    "        print(\"No numeric columns available for correlation analysis.\")\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T20:41:33.781109Z",
     "start_time": "2025-01-10T20:41:32.560780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(PlaneIssueYear_aggregated='1999', Model_aggregated='EMB-145', Dest_aggregated='Other', Origin_aggregated='IAH', Year=2008, Month=1, DayofMonth=1, DayOfWeek=2, CRSElapsedTime=122, ArrDelay=-4, DepDelay=0, Origin='IAH', Dest='TYS', Distance=772, TaxiOut=23, Manufacturer='EMBRAER', Model='EMB-145', AircraftType='Fixed Wing Multi-Engine', EngineType='Turbo-Jet', PlaneIssueYear='1999', DepTime_minutes=600, CRSDepTime_minutes=600, CRSArrTime_minutes=782, Month_cos=0.8660254037844387, Month_sin=0.49999999999999994, DayofMonth_cos=0.9795299412524945, DayofMonth_sin=0.20129852008866006, DayOfWeek_cos=-0.22252093395631434, DayOfWeek_sin=0.9749279121818236, EngineType_aggregated='Turbo-Jet', AircraftType_aggregated='Fixed Wing Multi-Engine', Manufacturer_aggregated='EMBRAER', Year_aggregated='2008', Origin_mean_enc=11.221490596320836, Dest_mean_enc=148.0, EngineType_index=1.0, EngineType_binary=SparseVector(8, {1: 1.0}), AircraftType_index=0.0, AircraftType_binary=SparseVector(5, {0: 1.0}), Manufacturer_index=1.0, Manufacturer_binary=SparseVector(38, {1: 1.0}), Model_mean_enc=21.66427432216906, Year_index=0.0, Year_binary=SparseVector(2, {0: 1.0}), PlaneIssueYear_mean_enc=10.071658664606904, ordinal_features_vector=DenseVector([0.5, 0.2013, 0.9749, 0.866, 0.9795, -0.2225]), quant_features_vector=DenseVector([122.0, 0.0, 772.0, 23.0, 600.0, 600.0, 782.0]), nominal_features_vector=SparseVector(57, {0: 11.2215, 1: 148.0, 3: 1.0, 10: 1.0, 16: 1.0, 53: 21.6643, 54: 1.0, 56: 10.0717}), features=SparseVector(70, {0: 122.0, 2: 772.0, 3: 23.0, 4: 600.0, 5: 600.0, 6: 782.0, 7: 11.2215, 8: 148.0, 10: 1.0, 17: 1.0, 23: 1.0, 60: 21.6643, 61: 1.0, 63: 10.0717, 64: 0.5, 65: 0.2013, 66: 0.9749, 67: 0.866, 68: 0.9795, 69: -0.2225}))\n",
      "Row(PlaneIssueYear_aggregated='1995', Model_aggregated='737-3H4', Dest_aggregated='Other', Origin_aggregated='Other', Year=2008, Month=1, DayofMonth=1, DayOfWeek=2, CRSElapsedTime=85, ArrDelay=137, DepDelay=146, Origin='MDW', Dest='MCI', Distance=405, TaxiOut=9, Manufacturer='BOEING', Model='737-3H4', AircraftType='Fixed Wing Multi-Engine', EngineType='Turbo-Fan', PlaneIssueYear='1995', DepTime_minutes=1075, CRSDepTime_minutes=1295, CRSArrTime_minutes=1380, Month_cos=0.8660254037844387, Month_sin=0.49999999999999994, DayofMonth_cos=0.9795299412524945, DayofMonth_sin=0.20129852008866006, DayOfWeek_cos=-0.22252093395631434, DayOfWeek_sin=0.9749279121818236, EngineType_aggregated='Turbo-Fan', AircraftType_aggregated='Fixed Wing Multi-Engine', Manufacturer_aggregated='BOEING', Year_aggregated='2008', Origin_mean_enc=148.0, Dest_mean_enc=148.0, EngineType_index=0.0, EngineType_binary=SparseVector(8, {0: 1.0}), AircraftType_index=0.0, AircraftType_binary=SparseVector(5, {0: 1.0}), Manufacturer_index=0.0, Manufacturer_binary=SparseVector(38, {0: 1.0}), Model_mean_enc=11.079108226455356, Year_index=0.0, Year_binary=SparseVector(2, {0: 1.0}), PlaneIssueYear_mean_enc=10.189869272814287, ordinal_features_vector=DenseVector([0.5, 0.2013, 0.9749, 0.866, 0.9795, -0.2225]), quant_features_vector=DenseVector([85.0, 146.0, 405.0, 9.0, 1075.0, 1295.0, 1380.0]), nominal_features_vector=SparseVector(57, {0: 148.0, 1: 148.0, 2: 1.0, 10: 1.0, 15: 1.0, 53: 11.0791, 54: 1.0, 56: 10.1899}), features=SparseVector(70, {0: 85.0, 1: 146.0, 2: 405.0, 3: 9.0, 4: 1075.0, 5: 1295.0, 6: 1380.0, 7: 148.0, 8: 148.0, 9: 1.0, 17: 1.0, 22: 1.0, 60: 11.0791, 61: 1.0, 63: 10.1899, 64: 0.5, 65: 0.2013, 66: 0.9749, 67: 0.866, 68: 0.9795, 69: -0.2225}))\n"
     ]
    }
   ],
   "source": [
    "TARGET_COLUMN = \"ArrDelay\"\n",
    "# Path to Parquet file\n",
    "FLIGHT_PARQUET_PATH = './data/flights.parquet'\n",
    "PLANES_PARQUET_PATH = './data/planes.parquet'\n",
    "PROCESSING_DIR = \"data/processing/\"\n",
    "# Path to schema file\n",
    "PLANE_SCHEMA_PATH = './data/plane-schema.json'\n",
    "FLIGHT_SCHEMA_PATH = './data/flight-schema.json'\n",
    "# Load paths\n",
    "FLIGHT_RAW_PATH = './data/*.csv.bz2'\n",
    "PLANE_RAW_PATH = './data/plane-data.csv'\n",
    "# Result paths\n",
    "PROCESSED_DIR = './data/processed/'\n",
    "PROCESSED_TRAIN_PARQUET = os.path.join(PROCESSED_DIR, \"train.parquet\")\n",
    "PROCESSED_TEST_PARQUET = os.path.join(PROCESSED_DIR, \"test.parquet\")\n",
    "PROCESSED_SCHEMA = os.path.join(PROCESSED_DIR, \"schema.json\")\n",
    "\n",
    "\n",
    "def load_csv_save_parquet(spark, raw_path, parquet_path, schema_path) -> DataFrame:\n",
    "    # Read csv\n",
    "    df = spark.read.csv(\n",
    "        raw_path,\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "\n",
    "    schema = df.schema\n",
    "    schema_json = schema.json()\n",
    "\n",
    "    # Write the schema JSON to a file\n",
    "    with open(schema_path, 'w') as f:\n",
    "        f.write(schema_json)\n",
    "\n",
    "    # Save DataFrame as Parquet for future use\n",
    "    df.repartition(1)\n",
    "    df.write.parquet(parquet_path)\n",
    "\n",
    "    df = spark.read.parquet(parquet_path, schema=schema)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_parquet(spark, parquet_path, schema_file) -> DataFrame:\n",
    "    with open(schema_file, 'r') as f:\n",
    "        schema_json = f.read()\n",
    "\n",
    "    # Deserialize the JSON string back into a StructType object\n",
    "    schema_from_file = StructType.fromJson(json.loads(schema_json))\n",
    "\n",
    "    df = spark.read.parquet(parquet_path, schema=schema_from_file)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load(spark, parquet_path, schema_file_path, wildcard_path) -> (DataFrame, DataFrame):\n",
    "    if os.path.exists(parquet_path):\n",
    "        # If Parquet exists, load it using the schema files\n",
    "        df = load_parquet(spark, parquet_path, schema_file_path)\n",
    "        df_planes = load_parquet(spark, PLANES_PARQUET_PATH, PLANE_SCHEMA_PATH)\n",
    "    else:\n",
    "        # If Parquet file does not exist, read CSV files and save as Parquet\n",
    "        df = load_csv_save_parquet(spark, wildcard_path, parquet_path, schema_file_path)\n",
    "        df_planes = load_csv_save_parquet(spark, PLANE_RAW_PATH, PLANES_PARQUET_PATH, PLANE_SCHEMA_PATH)\n",
    "    return df, df_planes\n",
    "\n",
    "\n",
    "def custom_polar_time_encode(df):\n",
    "    print(f\"Transforming Month, DayofMonth and DayofWeek to polar coordinates.\")\n",
    "\n",
    "    def polar_encoding(value, max_value):\n",
    "        frac = value / max_value\n",
    "        circle = 2 * pi\n",
    "        angle = frac * circle\n",
    "        return cos(angle), sin(angle)\n",
    "\n",
    "    # Register UDF for polar encoding\n",
    "    polar_udf = udf(polar_encoding, \"struct<cos:double, sin:double>\")\n",
    "\n",
    "    # Apply polar encoding on 'Month', 'DayofMonth', 'DayOfWeek'\n",
    "    df = df.withColumn(\"Month_polar\", polar_udf(col(\"Month\"), lit(12))) \\\n",
    "        .withColumn(\"DayofMonth_polar\", polar_udf(col(\"DayofMonth\"),\n",
    "                                                  when(col(\"Month\") == 2, lit(28))  # February (can adjust for leap year\n",
    "                                                  # if needed)\n",
    "                                                  .when(col(\"Month\").isin([4, 6, 9, 11]),\n",
    "                                                        lit(30))  # Months with 30 days\n",
    "                                                  .otherwise(lit(31)))) \\\n",
    "        .withColumn(\"DayOfWeek_polar\", polar_udf(col(\"DayOfWeek\"), lit(7)))\n",
    "    # df = df.drop(*[\"DayofMonth\", \"DayOfWeek\", \"Month\"])\n",
    "\n",
    "    # Subdivide feature pairs into two columns each\n",
    "    df = df.withColumn(\"Month_cos\", col(\"Month_polar.cos\")) \\\n",
    "        .withColumn(\"Month_sin\", col(\"Month_polar.sin\")) \\\n",
    "        .withColumn(\"DayofMonth_cos\", col(\"DayofMonth_polar.cos\")) \\\n",
    "        .withColumn(\"DayofMonth_sin\", col(\"DayofMonth_polar.sin\")) \\\n",
    "        .withColumn(\"DayOfWeek_cos\", col(\"DayOfWeek_polar.cos\")) \\\n",
    "        .withColumn(\"DayOfWeek_sin\", col(\"DayOfWeek_polar.sin\"))\n",
    "\n",
    "    df = df.drop(*[\"DayofMonth_polar\", \"DayOfWeek_polar\", \"Month_polar\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def static_preprocess(df, df_planes):\n",
    "    df_planes = df_planes.withColumnRenamed(\"tailnum\", \"TailNum\")\n",
    "    df = df.join(df_planes, on=\"TailNum\", how=\"inner\")\n",
    "\n",
    "    print(\"Schema before static preprocessing\")\n",
    "    df.printSchema()\n",
    "\n",
    "    forbidden_cols = [\n",
    "        \"ArrTime\",\n",
    "        \"ActualElapsedTime\",\n",
    "        \"AirTime\",\n",
    "        \"TaxiIn\",\n",
    "        \"Diverted\",\n",
    "        \"CarrierDelay\",\n",
    "        \"WeatherDelay\",\n",
    "        \"NASDelay\",\n",
    "        \"SecurityDelay\",\n",
    "        \"LateAircraftDelay\"\n",
    "    ]\n",
    "    df = df.drop(*forbidden_cols)\n",
    "\n",
    "    target_column = \"ArrDelay\"\n",
    "\n",
    "    # List of Ordinal features\n",
    "    cyclic_ordinal_time = [\n",
    "        'Month',\n",
    "        'DayofMonth',\n",
    "        'DayOfWeek'\n",
    "    ]\n",
    "    non_cyclic_ordinal_time = ['Year', 'PlaneIssueYear']\n",
    "\n",
    "    # List of Time features\n",
    "    quant_time_features = [\n",
    "        'DepTime',\n",
    "        'CRSDepTime',\n",
    "        'CRSArrTime'\n",
    "    ]\n",
    "\n",
    "    # List of Quantitative features\n",
    "    quantitative_features = [\n",
    "        'CRSElapsedTime',\n",
    "        'DepDelay',\n",
    "        'Distance',\n",
    "        'TaxiOut'\n",
    "    ]\n",
    "\n",
    "    # List of Nominal features\n",
    "    nominal_features = [\n",
    "        'UniqueCarrier',\n",
    "        'FlightNum',\n",
    "        'TailNum',\n",
    "        'Origin',\n",
    "        'Dest',\n",
    "        'Cancelled',\n",
    "        'CancellationCode',\n",
    "        'EngineType',\n",
    "        'AircraftType',\n",
    "        'Manufacturer',\n",
    "        'Model',\n",
    "        \"issue_date\", \"status\",\n",
    "        \"type\"\n",
    "    ]\n",
    "\n",
    "    # WE ARE PREDICTING DELAY. REMOVE CANCELLED FLIGHTS\n",
    "    df = df.filter(\"Cancelled != 1\")\n",
    "\n",
    "    # DROP NOMINALS WITH TOO MANY GROUPS OR THAT ARE USELESS\n",
    "    useless_fea = [\"TailNum\", \"FlightNum\", \"UniqueCarrier\", \"CancellationCode\", \"Cancelled\", \"issue_date\", \"status\",\n",
    "                   \"type\"]\n",
    "    for fea in useless_fea:\n",
    "        print(f\"Discarding {fea}.\")\n",
    "        nominal_features.remove(fea)\n",
    "    df = df.drop(*useless_fea)\n",
    "\n",
    "    # RENAME VARIABLES\n",
    "    df = df.withColumnRenamed(\"year\", \"PlaneIssueYear\")\n",
    "    df = df.withColumnRenamed(\"engine_type\", \"EngineType\")\n",
    "    df = df.withColumnRenamed(\"aircraft_type\", \"AircraftType\")\n",
    "    df = df.withColumnRenamed(\"model\", \"Model\")\n",
    "    df = df.withColumnRenamed(\"manufacturer\", \"Manufacturer\")\n",
    "\n",
    "    # CAST QUANTITATIVE COLUMNS TO NUMERIC, SOME ARE STRINGS\n",
    "    for column in quantitative_features + [target_column]:\n",
    "        print(f\"Forcing {column} to be read as integer.\")\n",
    "        df = df.withColumn(column, col(column).cast(IntegerType()))\n",
    "    df = df.dropna(subset=[target_column])\n",
    "    null_count = df.filter(col(target_column).isNull()).count()\n",
    "    print(f\"Number of nulls in {target_column}: {null_count}\")\n",
    "\n",
    "    # CAST HHMM COLUMNS TO MINUTE QUANTITIES\n",
    "    for column in quant_time_features:  # They are strings hhmm\n",
    "        print(f\"Casting {column} from hhmm to minutes (integer).\")\n",
    "        df = df.withColumn(\n",
    "            column + \"_minutes\",\n",
    "            (F.col(column).substr(1, 2).cast(\"int\") * 60 + F.col(column).substr(3, 2).cast(\"int\"))\n",
    "        )\n",
    "        quantitative_features.append(column + \"_minutes\")\n",
    "    df = df.drop(*quant_time_features)\n",
    "\n",
    "    df = custom_polar_time_encode(df)\n",
    "    ordinal_features = []\n",
    "    ordinal_features += [fea + \"_sin\" for fea in cyclic_ordinal_time]\n",
    "    ordinal_features += [fea + \"_cos\" for fea in cyclic_ordinal_time]\n",
    "\n",
    "    return df, quantitative_features, ordinal_features, nominal_features + non_cyclic_ordinal_time\n",
    "\n",
    "\n",
    "def train_preprocess(df, nominal_features, ordinal_features, quantitative_features, dir_save_params,\n",
    "                     cardinality_threshold, frequency_threshold, high_cardinality_strategy):\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    # -------------------------------- IMPUTER --------------------------------\n",
    "    # This should be the column, the values considered nulls, and the value to be used to fill\n",
    "\n",
    "    print(\"Analyzing medians\")\n",
    "    imputer_maps = {\n",
    "        fea: {'extra_nulls': [],\n",
    "              'fill_value': df.approxQuantile(col=fea, probabilities=[0.5], relativeError=0.025)[0]} for fea in\n",
    "        quantitative_features\n",
    "    }\n",
    "    print(\"Current imputing dictionary: \")\n",
    "    print(imputer_maps)\n",
    "    print(\"Analyzing modes\")\n",
    "    imputer_maps.update({\n",
    "        fea: {'extra_nulls': ['None'],\n",
    "              'fill_value': df.groupby(fea).count().orderBy(\"count\", ascending=False).first()[0]} for fea in\n",
    "        ordinal_features + nominal_features\n",
    "    })\n",
    "    print(\"Filling dictionary: \")\n",
    "    print(imputer_maps)\n",
    "    # Convert to JSON and save it\n",
    "    json_data = json.dumps(imputer_maps, indent=4)\n",
    "\n",
    "    # Save to a file\n",
    "    with open(os.path.join(dir_save_params, 'imputer_maps.json'), 'w') as f:\n",
    "        f.write(json_data)\n",
    "\n",
    "    # ----------------------------- NOMINAL ENCODER ----------------------------\n",
    "\n",
    "    def get_sufficiently_frequent(df, fea, frequency_threshold=frequency_threshold):\n",
    "        total_count = df.count()\n",
    "\n",
    "        # Group by the column and calculate the normalized frequency\n",
    "        proportions = df.groupBy(fea).agg(\n",
    "            (F.count(\"*\") / total_count).alias(f\"{fea}_frequency\")\n",
    "        )\n",
    "        result = proportions.filter(F.col(f\"{fea}_frequency\") > frequency_threshold).select(fea).collect()\n",
    "        result = [row[fea] for row in result]\n",
    "        return result\n",
    "\n",
    "    feature_to_sufficiently_frequent = {\n",
    "        fea: get_sufficiently_frequent(df, fea) for fea in nominal_features\n",
    "    }\n",
    "    print(\"Sufficiently frequent values per feature: \")\n",
    "    print(feature_to_sufficiently_frequent)\n",
    "\n",
    "    # Map between feature and the encoder and new column name\n",
    "    nominal_encode_type = {}\n",
    "    nominal_encoders = {}\n",
    "    new_nominal = []\n",
    "    for fea in nominal_features:\n",
    "        elems_to_preserve = feature_to_sufficiently_frequent[fea]\n",
    "        df = df.withColumn(\n",
    "            f\"{fea}_aggregated\",\n",
    "            (F.when(~F.col(fea).isin(elems_to_preserve), lit(\"Other\")).otherwise(F.col(fea)))\n",
    "        )\n",
    "\n",
    "        if len(elems_to_preserve) + 1 <= cardinality_threshold:\n",
    "            print(f\"Performing One-Hot-Encoding to feature {fea}\")\n",
    "            indexer = StringIndexer(inputCol=f\"{fea}_aggregated\", outputCol=f\"{fea}_index\", handleInvalid='keep')\n",
    "            encoder = OneHotEncoder(inputCol=f\"{fea}_index\", outputCol=f\"{fea}_binary\", handleInvalid='keep',\n",
    "                                    dropLast=True)\n",
    "            pipeline = Pipeline(stages=[indexer, encoder])\n",
    "            pipeline_model = pipeline.fit(df)\n",
    "            nominal_encode_type[f\"{fea}_aggregated\"] = \"binary\"\n",
    "            new_nominal.append(f\"{fea}_binary\")\n",
    "            pipeline_model.save(os.path.join(dir_save_params, f'{fea}_aggregated_encoder'))\n",
    "        elif high_cardinality_strategy == \"ignore\":\n",
    "            print(f\"Ignoring feature {fea}\")\n",
    "        elif high_cardinality_strategy == \"mean\":\n",
    "            print(f\"Performing Mean-Target-Encoding to feature {fea}\")\n",
    "            mapping_df = df.groupBy(f\"{fea}_aggregated\").agg(F.avg(\"ArrDelay\").alias(f\"{fea}_mean_enc\"))\n",
    "            if \"Other\" not in mapping_df.select(f\"{fea}_aggregated\").distinct().collect():\n",
    "                mean = float(df.groupBy(TARGET_COLUMN).agg(F.avg(\"ArrDelay\")).collect()[0][0])\n",
    "                print(mean)\n",
    "                new_row = Row(f\"{fea}_aggregated\", f\"{fea}_mean_enc\")(\"Other\", mean)\n",
    "                print(new_row)\n",
    "                # Convert the new row to a DataFrame with the same schema as mapping_df\n",
    "                new_row_df = spark.createDataFrame([new_row], mapping_df.schema)\n",
    "                print(new_row_df.show())\n",
    "                mapping_df = mapping_df.union(new_row_df)\n",
    "                print(mapping_df.show())\n",
    "            mapping_df.write.csv(os.path.join(dir_save_params, f'{fea}_aggregated_encoder.csv'), header=True)\n",
    "            new_nominal.append(f\"{fea}_mean_enc\")\n",
    "            nominal_encode_type[f\"{fea}_aggregated\"] = \"mean\"\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Not implemented strategy {high_cardinality_strategy}\")\n",
    "\n",
    "    print(\"Feature to encoder types:\")\n",
    "    print(nominal_encode_type)\n",
    "    print(\"Final nominal variables:\")\n",
    "    print(new_nominal)\n",
    "\n",
    "    # Convert to JSON and save it\n",
    "    json_data = json.dumps(nominal_encode_type, indent=4)\n",
    "    with open(os.path.join(dir_save_params, 'encode_types.json'), 'w') as f:\n",
    "        f.write(json_data)\n",
    "\n",
    "    json_data = json.dumps(feature_to_sufficiently_frequent, indent=4)\n",
    "    with open(os.path.join(dir_save_params, 'non_aggregated.json'), 'w') as f:\n",
    "        f.write(json_data)\n",
    "\n",
    "    # -------------------------------- VECTORIZER --------------------------------\n",
    "    # Quantitative feature assembly\n",
    "    quant_assembler = VectorAssembler(\n",
    "        inputCols=quantitative_features,\n",
    "        outputCol=\"quant_features_vector\"\n",
    "    )\n",
    "\n",
    "    # Assemble encoded nominal features\n",
    "    nominal_assembler = VectorAssembler(\n",
    "        inputCols=new_nominal,\n",
    "        outputCol=\"nominal_features_vector\"\n",
    "    )\n",
    "\n",
    "    ordinal_assembler = VectorAssembler(\n",
    "        inputCols=ordinal_features,\n",
    "        outputCol=\"ordinal_features_vector\"\n",
    "    )\n",
    "\n",
    "    # Final feature vector\n",
    "    final_assembler = VectorAssembler(\n",
    "        inputCols=[\"quant_features_vector\", \"nominal_features_vector\", \"ordinal_features_vector\"],\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "\n",
    "    # Create a pipeline\n",
    "    pipeline = Pipeline(stages=[ordinal_assembler,\n",
    "                                quant_assembler,\n",
    "                                nominal_assembler,\n",
    "                                final_assembler\n",
    "                                ])\n",
    "    vectorizer = pipeline.fit(df)\n",
    "    vectorizer.save(os.path.join(dir_save_params, 'vectorizer'))\n",
    "    # -------------------------------- VECTORIZER --------------------------------\n",
    "\n",
    "\n",
    "def dynamic_preprocess(df, nominal_features, ordinal_features, quantitative_features, dir_load_params):\n",
    "    # -------------------------------- IMPUTER --------------------------------\n",
    "    with open(os.path.join(dir_load_params, 'imputer_maps.json'), 'r') as f:\n",
    "        imputer_maps = json.load(f)\n",
    "\n",
    "    for fea in quantitative_features + ordinal_features + nominal_features:\n",
    "        if len(imputer_maps[fea]['extra_nulls']) > 0:\n",
    "            df = df.withColumn(fea, when(df[fea].isin(imputer_maps[fea]['extra_nulls']), lit(None)).otherwise(df[fea]))\n",
    "\n",
    "        if df.filter(col(fea).isNull()).count() > 0:\n",
    "            value = imputer_maps[fea]['fill_value']\n",
    "            print(f\"Imputing {fea} with {value}\")\n",
    "            df = df.fillna(value, subset=fea)\n",
    "\n",
    "    # ----------------------------- NOMINAL ENCODER ----------------------------\n",
    "    with open(os.path.join(dir_load_params, 'encode_types.json'), 'r') as f:\n",
    "        encode_types = json.load(f)\n",
    "    with open(os.path.join(dir_load_params, 'non_aggregated.json'), 'r') as f:\n",
    "        fea_2_non_aggregated = json.load(f)\n",
    "\n",
    "    for fea, non_aggregated in fea_2_non_aggregated.items():\n",
    "        df = df.withColumn(\n",
    "            f\"{fea}_aggregated\",\n",
    "            (F.when(~F.col(fea).isin(non_aggregated), lit(\"Other\")).otherwise(F.col(fea)))\n",
    "        )\n",
    "\n",
    "    for fea, encode_type in encode_types.items():\n",
    "        if encode_type == 'binary':\n",
    "            encoder = PipelineModel.load(os.path.join(dir_load_params, f'{fea}_encoder'))\n",
    "            df = encoder.transform(df)\n",
    "        elif encode_type == 'mean':\n",
    "            encoder = SparkSession.builder.getOrCreate().read.csv(os.path.join(dir_load_params, f'{fea}_encoder.csv'),\n",
    "                                                                  header=True, inferSchema=True)\n",
    "            df = df.join(encoder, on=fea, how='left')\n",
    "            new_var = f\"{fea}_mean_enc\".replace(\"_aggregated\", \"\")\n",
    "            imput_value = encoder.filter(encoder[fea] == \"Other\").select(new_var).collect()[0][0]\n",
    "            print(f\"Using the following encoder for {fea}\")\n",
    "            print(encoder.show(10))\n",
    "            print(f\"Imputing unrecognized values in {fea} with 'Other'->{imput_value}\")\n",
    "            df = df.fillna(imput_value, subset=new_var)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Not implemented encode type {encode_type}\")\n",
    "\n",
    "    # ------------------------------ VECTORIZER --------------------------------\n",
    "    vectorizer = PipelineModel.load(os.path.join(dir_load_params, 'vectorizer'))\n",
    "    df = vectorizer.transform(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def assure_existence_directory(directory_path):\n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "\n",
    "\n",
    "def preprocess_fit_and_transform(df, df_planes, dir_save_params=\"./data/\"):\n",
    "    df, quantitative_features, ordinal_features, nominal_features = static_preprocess(df, df_planes)\n",
    "\n",
    "    if len(os.listdir(dir_save_params)) == 0:\n",
    "        print(\"TRAINING DYNAMIC PREPROCESSING PARAMETERS\")\n",
    "        train_preprocess(df, nominal_features, ordinal_features, quantitative_features, dir_save_params,\n",
    "                         cardinality_threshold=10, frequency_threshold=0.02, high_cardinality_strategy=\"mean\")\n",
    "    else:\n",
    "        print(\"DYNAMIC PREPROCESSING PARAMETERS FOUND. SKIPPING LEARNING.\")\n",
    "    df = dynamic_preprocess(df, nominal_features, ordinal_features, quantitative_features, dir_save_params)\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_and_preprocess(df, df_planes, train_frac=0.8, dir_save_params=\"./data/\"):\n",
    "    train_df, test_df = df.randomSplit([train_frac, 1 - train_frac], seed=42)\n",
    "    train_df = preprocess_fit_and_transform(train_df, df_planes, dir_save_params=dir_save_params)\n",
    "\n",
    "    print(\"TESTING DATA PROCESSING\")\n",
    "    test_df, quantitative_features, ordinal_features, nominal_features = static_preprocess(test_df, df_planes)\n",
    "    test_df = dynamic_preprocess(test_df, nominal_features, ordinal_features, quantitative_features, dir_save_params)\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "# THIS IS THE FUNCTION TO USE TO PREPROCESS VALIDATION DATA PASSED THROUGH CONSOLE <-------------------------------------\n",
    "def validation_preprocess(df, dir_save_params=\"./data/\"):\n",
    "    spark = SparkSession.builder.appName(\"MachineLearningProject\").getOrCreate()\n",
    "    df_planes = load_parquet(spark, PLANES_PARQUET_PATH, PLANE_SCHEMA_PATH)\n",
    "    df, quantitative_features, ordinal_features, nominal_features = static_preprocess(df, df_planes)\n",
    "    df = dynamic_preprocess(df, nominal_features, ordinal_features, quantitative_features, dir_save_params)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_split_and_preprocess(n_partitions=10, debug=False):\n",
    "    spark = (SparkSession.builder.appName(\"MachineLearningProject\") # Change this as needed\n",
    "             .config(\"spark.executor.memory\", \"4g\")\n",
    "             .config(\"spark.driver.memory\", \"48g\")\n",
    "             .config(\"spark.memory.fraction\", \"0.8\")\n",
    "             .config(\"spark.memory.storageFraction\", \"0.3\")\n",
    "             .config(\"spark.driver.maxResultSize\", \"4g\")\n",
    "             .config(\"spark.sql.caseSensitive\", \"true\")\n",
    "             .config(\"spark.sql.debug.maxToStringFields\", \"200\")\n",
    "             # .config(\"spark.local.dir\", \"./temp/\")\n",
    "             .getOrCreate())\n",
    "\n",
    "    if not os.path.exists(PROCESSED_TRAIN_PARQUET):\n",
    "        df, df_planes = load(spark, FLIGHT_PARQUET_PATH, PLANE_SCHEMA_PATH, FLIGHT_RAW_PATH)\n",
    "        df = df.repartition(n_partitions)\n",
    "\n",
    "        if debug:\n",
    "            fraction = 0.01  # Adjust the fraction to select 10% of rows\n",
    "            df = df.sample(withReplacement=True, fraction=fraction)\n",
    "            df = df.repartition(1)\n",
    "\n",
    "        # train_df, test_df = complete_preprocess(df, df_planes, train_frac=0.8)\n",
    "        assure_existence_directory(PROCESSING_DIR)\n",
    "        train_df, test_df = split_and_preprocess(df, df_planes, train_frac=0.8, dir_save_params=PROCESSING_DIR)\n",
    "        print(\"Finished preprocessing\")\n",
    "        print(train_df.head())\n",
    "        print(test_df.head())\n",
    "\n",
    "        print(f\"Saving schema to {PROCESSED_SCHEMA}\")\n",
    "        assure_existence_directory(PROCESSED_DIR)\n",
    "        schema_json = train_df.schema.json()\n",
    "        with open(PROCESSED_SCHEMA, 'w') as f:\n",
    "            f.write(schema_json)\n",
    "        test_df.write.mode('overwrite').parquet(PROCESSED_TEST_PARQUET)\n",
    "        train_df.write.mode('overwrite').parquet(PROCESSED_TRAIN_PARQUET)\n",
    "    else:\n",
    "        with open(PROCESSED_SCHEMA, 'r') as f:\n",
    "            schema_json = f.read()\n",
    "\n",
    "        schema = StructType.fromJson(json.loads(schema_json))\n",
    "\n",
    "        test_df = spark.read.parquet(PROCESSED_TEST_PARQUET, schema=schema)\n",
    "        train_df = spark.read.parquet(PROCESSED_TRAIN_PARQUET, schema=schema)\n",
    "\n",
    "        print(test_df.head())\n",
    "        print(train_df.head())\n",
    "    spark.stop()\n",
    "\n",
    "load_split_and_preprocess(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<<<<<<< REMOTE CELL DELETED >>>>>>>\n",
    "# =================== 3. Data Processing ===================\n",
    "def process_data(data, mode):\n",
    "    \"\"\"\n",
    "    Process the dataset: handle missing values and perform feature engineering.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): Spark DataFrame to process.\n",
    "        mode (str): Mode of operation (\"train\" or \"predict\").\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Processed Spark DataFrame with new features added.\n",
    "    \"\"\"\n",
    "    # Validate the target variable for training mode\n",
    "    if mode == \"train\" and \"ArrDelay\" not in data.columns:\n",
    "        raise ValueError(\"The target variable 'ArrDelay' is missing.\")\n",
    "\n",
    "    # Handle missing values\n",
    "    if mode == \"train\":\n",
    "        # Drop rows where the target variable or features are null\n",
    "        data = data.dropna(subset=[\"ArrDelay\"])\n",
    "\n",
    "    # Example: Fill null values in specific columns with a default value\n",
    "    # Replace 'column_name' with actual column names as needed\n",
    "    # Uncomment this if specific columns require filling\n",
    "    # data = data.fillna({\"column_name\": 0})\n",
    "\n",
    "    # Transform special variables\n",
    "    # Feature engineering: Create time-based features\n",
    "    if \"DepTime\" in data.columns:\n",
    "        data = data.withColumn(\"DepHour\", (col(\"DepTime\") / 100).cast(\"int\"))  # Extract hour from departure time\n",
    "\n",
    "    if \"FlightDate\" in data.columns:\n",
    "        data = data.withColumn(\"DayOfWeek\", date_format(col(\"FlightDate\"), \"u\").cast(\"int\"))  # Convert to day of the week\n",
    "\n",
    "    # Feature engineering: Create flight distance categories\n",
    "    if \"Distance\" in data.columns:\n",
    "        data = data.withColumn(\n",
    "            \"DistanceCategory\",\n",
    "            when(col(\"Distance\") < 500, \"Short\")  # Short flights\n",
    "            .when((col(\"Distance\") >= 500) & (col(\"Distance\") < 1500), \"Medium\")  # Medium flights\n",
    "            .otherwise(\"Long\")  # Long flights\n",
    "        )\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T17:51:46.340291Z",
     "start_time": "2025-01-10T17:51:46.326291Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the pipeline workflow.\n",
    "    Accepts command-line arguments for dynamic input/output handling.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Flight Delay Prediction Application\")\n",
    "    parser.add_argument(\"--mode\", type=str, required=True, choices=[\"train\", \"predict\"], help=\"Mode: train or predict\")\n",
    "    parser.add_argument(\"--input\", type=str, required=True, help=\"Path to input CSV file\")\n",
    "    parser.add_argument(\"--model\", type=str, required=True, help=\"Path to save/load the model\")\n",
    "    parser.add_argument(\"--output\", type=str, help=\"Path to save predictions (required for predict mode)\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Start Spark Session\n",
    "    spark = SparkSession.builder.appName(\"FlightDelayPipeline\").getOrCreate()\n",
    "\n",
    "    try:\n",
    "        # Workflow\n",
    "        data = load_data(spark, args.input, args.mode)  # Load the dataset\n",
    "        data = eda(data)\n",
    "        # data = validation_preprocess(data, args.mode)        # Preprocess the dataset\n",
    "        # pipeline, _ = feature_engineering(data)        # Perform feature engineering\n",
    "\n",
    "        if args.mode == \"train\":\n",
    "            # Train the model, evaluate it, and optionally save it\n",
    "            metrics = build_and_train_model(data, pipeline, args.model)\n",
    "            print(f\"Training completed. Evaluation metrics: {metrics}\")\n",
    "        elif args.mode == \"predict\":\n",
    "            if not args.output:\n",
    "                raise ValueError(\"Output path is required for prediction mode.\")\n",
    "            # Use the trained model to generate predictions\n",
    "            predict(data, args.model, args.output)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Add this block to execute the script when running it as a standalone script\n",
    "#if __name__ == \"__main__\":\n",
    "#    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T17:51:46.388297Z",
     "start_time": "2025-01-10T17:51:46.376292Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.functions import col, isnan\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "def test_train_and_predict(processed_schema_path, processed_train_path_parquet, processed_test_path_parquet, model_path):\n",
    "    \"\"\"\n",
    "    Function to test the build_and_train_model and predict functions using train and test datasets.\n",
    "\n",
    "    Args:\n",
    "        processed_schema_path (str): Path to the schema JSON file.\n",
    "        processed_train_path_parquet (str): Path to the train.parquet file.\n",
    "        processed_test_path_parquet (str): Path to the test.parquet file.\n",
    "        model_path (str): Path to save or load the trained model.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Configuración de Spark\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Optimización con recursos limitados\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.executor.memory\", \"2g\") \\\n",
    "        .config(\"spark.driver.memory\", \"2g\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "        .config(\"spark.python.worker.timeout\", \"600\") \\\n",
    "        .config(\"spark.network.timeout\", \"600s\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "        \n",
    "    pipeline = Pipeline(stages=[])\n",
    "        \n",
    "    try:\n",
    "        # Carga el esquema desde el archivo JSON\n",
    "        print(\"Loading schema from JSON...\")\n",
    "        with open(processed_schema_path, 'r') as f:\n",
    "            schema_json = f.read()\n",
    "\n",
    "        schema = StructType.fromJson(json.loads(schema_json))\n",
    "\n",
    "        # Carga los DataFrames de Parquet usando el esquema\n",
    "        print(\"Loading train and test datasets from Parquet...\")\n",
    "        train_df = spark.read.parquet(processed_train_path_parquet, schema=schema)\n",
    "        test_df = spark.read.parquet(processed_test_path_parquet, schema=schema)\n",
    "\n",
    "\n",
    "        train_df = train_df.sample(fraction=0.1)  # Usa una muestra del 10%\n",
    "        train_df = train_df.repartition(10)  # Reparticionar para evitar carga de memoria\n",
    "\n",
    "\n",
    "        # Persistir los DataFrames en disco\n",
    "        train_df.persist(StorageLevel.DISK_ONLY)\n",
    "        test_df.persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "        # Confirmación de datos cargados\n",
    "        print(\"Train and test datasets loaded successfully!\")\n",
    "        print(f\"Train dataset count: {train_df.count()}\")\n",
    "        print(f\"Test dataset count: {test_df.count()}\")\n",
    "\n",
    "        # train_df = train_df.repartition(200)  # Ajusta el número de particiones según el tamaño de los datos\n",
    "\n",
    "        # Train the model\n",
    "        print(\"Training the model...\")\n",
    "        metrics = build_and_train_model(train_df, pipeline, model_path)\n",
    "        print(f\"Training completed. Metrics: {metrics}\")\n",
    "\n",
    "        # Predict using the trained model\n",
    "        print(\"Making predictions on the test dataset...\")\n",
    "        output_path = model_path + \"_predictions\"\n",
    "        predict(test_df, model_path, output_path)\n",
    "        print(f\"Predictions saved to: {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(\"An error occurred:\", e)\n",
    "        traceback.print_exc()\n",
    "\n",
    "# # Rutas de ejemplo\n",
    "# processed_schema_path = \"data/processed/schema.json\"  # Ruta al archivo JSON del esquema\n",
    "# processed_train_path_parquet = \"data/processed/train.parquet\"  # Ruta al archivo Parquet de train\n",
    "# processed_test_path_parquet = \"data/processed/test.parquet\"  # Ruta al archivo Parquet de test\n",
    "# model_path = \"data/models/trained_model\"\n",
    "\n",
    "# # Llamada a la función\n",
    "# test_train_and_predict(processed_schema_path, processed_train_path_parquet, processed_test_path_parquet, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T18:40:02.141628Z",
     "start_time": "2025-01-10T18:40:02.126630Z"
    }
   },
   "outputs": [],
   "source": [
    "def validate(data, model_path, output_path):\n",
    "    \"\"\"\n",
    "    Load the trained model and generate predictions for the given data.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): Spark DataFrame for predictions.\n",
    "        model_path (str): Path to load the trained model.\n",
    "        output_path (str): Path to save predictions (CSV).\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load the trained model\n",
    "    \n",
    "    for model_name in os.listdir(model_path):\n",
    "        model_folder = os.path.join(model_path, model_name)\n",
    "        print(f\"Loading model: {model_folder}\")\n",
    "        model = PipelineModel.load(model_folder)\n",
    "        name = model_name\n",
    "        # # Drop the existing 'prediction' column if it exists\n",
    "        # if \"prediction\" in data.columns:\n",
    "        #     print(\"Dropping existing 'prediction' column...\")\n",
    "        #     data = data.drop(\"prediction\")\n",
    "    \n",
    "        # Make predictions on the input data\n",
    "        print(\"Validating model...\")\n",
    "        predictions = model.transform(data)\n",
    "        rmse_evaluator = RegressionEvaluator(labelCol=\"ArrDelay\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "        metrics = {}\n",
    "        metrics['rmse'] = rmse_evaluator.evaluate(predictions)\n",
    "        print(f\"{name} - Root Mean Square Error (RMSE) on test data: {metrics['rmse']}\")\n",
    "\n",
    "        # Mean Absolute Error (MAE)\n",
    "        mae_evaluator = RegressionEvaluator(labelCol=\"ArrDelay\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "        metrics['mae'] = mae_evaluator.evaluate(predictions)\n",
    "        print(f\"{name} - Mean Absolute Error (MAE) on test data: {metrics['mae']}\")\n",
    "\n",
    "        # R-Squared (R²)\n",
    "        r2_evaluator = RegressionEvaluator(labelCol=\"ArrDelay\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "        metrics['r2'] = r2_evaluator.evaluate(predictions)\n",
    "        print(f\"{name} - R-Squared (R²) on test data: {metrics['r2']}\")\n",
    "        # Save predictions to the specified output path\n",
    "        \n",
    "        out_csv_path = os.path.join(output_path, name + \"_pred.csv\")\n",
    "        print(f\"Saving predictions to {out_csv_path}\")\n",
    "        old_columns = [col for col in predictions.columns if \"_\" not in col]\n",
    "        predictions = predictions.select(old_columns).drop(*[\"features\", \"scaledFeatures\"])\n",
    "        os.makedirs(output_path, exist_ok=True)  # Ensure output directory exists\n",
    "        predictions.write.mode(\"overwrite\").csv(out_csv_path, header=True)\n",
    "        print(f\"Predictions saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T19:05:18.069434Z",
     "start_time": "2025-01-10T19:05:18.048114Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_and_train_model(train_df, model_save_path=None):\n",
    "    \"\"\"\n",
    "    Build, train, evaluate, and optionally save the model using cross-validation with three models.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): Spark DataFrame with features and labels..\n",
    "        model_save_path (str): Path to save the trained model (optional).\n",
    "    \n",
    "    Returns:\n",
    "        dict: Evaluation metrics for the trained model.\n",
    "    \"\"\"\n",
    "    # Split data into training and testing sets\n",
    "    train_data, test_data = train_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "    if \"prediction\" in train_data.columns:\n",
    "        train_data = train_data.drop(\"prediction\")\n",
    "    \n",
    "    # Initialize metrics dictionary\n",
    "    all_metrics = {}\n",
    "    \n",
    "    \n",
    "    # Define the models: RandomForestRegressor, DecisionTreeRegressor, LinearRegression\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withMean=True, withStd=True)\n",
    "    rf = RandomForestRegressor(featuresCol=\"scaledFeatures\", labelCol=\"ArrDelay\")\n",
    "    dt = DecisionTreeRegressor(featuresCol=\"scaledFeatures\", labelCol=\"ArrDelay\")\n",
    "    lr = LinearRegression(featuresCol=\"scaledFeatures\", labelCol=\"ArrDelay\")\n",
    "    \n",
    "    models = {\n",
    "        'Linear_Regression': lr, 'Decision_Tree': dt, 'Random_Forest': rf\n",
    "    }\n",
    "    model_names = list(models.keys())\n",
    "    # model_names = ['Linear_Regression']\n",
    "    best = 0 \n",
    "    best_model = None \n",
    "    for name in model_names:\n",
    "        model = models[name]\n",
    "        print(f\"Training {name} model...\")\n",
    "        \n",
    "        # Add the current model to the pipeline\n",
    "        pipeline = Pipeline(stages=[scaler, model])\n",
    "\n",
    "        # Hyperparameter tuning with cross-validation for the current model\n",
    "        param_grid_builder = ParamGridBuilder()\n",
    "\n",
    "        if isinstance(model, RandomForestRegressor):\n",
    "            param_grid_builder.addGrid(model.numTrees, [10, 50, 100])\n",
    "        elif isinstance(model, DecisionTreeRegressor):\n",
    "            param_grid_builder.addGrid(model.maxDepth, [5, 10, 20])\n",
    "        elif isinstance(model, LinearRegression):\n",
    "            param_grid_builder.addGrid(model.regParam, [0.1, 0.3, 0.5])\n",
    "\n",
    "        # Construir la grilla de parámetros\n",
    "        param_grid = param_grid_builder.build()\n",
    "\n",
    "\n",
    "        evaluator = RegressionEvaluator(labelCol=\"ArrDelay\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "        # Set up cross-validation\n",
    "        cv = CrossValidator(\n",
    "            estimator=pipeline,\n",
    "            estimatorParamMaps=param_grid,\n",
    "            evaluator=evaluator,\n",
    "            numFolds=5\n",
    "        )\n",
    "\n",
    "        # Train the model with cross-validation\n",
    "        cv_model = cv.fit(train_data)\n",
    "\n",
    "        if \"prediction\" in train_df.columns:\n",
    "            data = data.drop(\"prediction\")\n",
    "\n",
    "        # Generate predictions on the test dataset\n",
    "        predictions = cv_model.transform(test_data)\n",
    "\n",
    "        # Evaluate the model using multiple metrics\n",
    "        metrics = {}\n",
    "        # Root Mean Square Error (RMSE)\n",
    "        rmse_evaluator = RegressionEvaluator(labelCol=\"ArrDelay\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "        metrics['rmse'] = rmse_evaluator.evaluate(predictions)\n",
    "        print(f\"{name} - Root Mean Square Error (RMSE) on test data: {metrics['rmse']}\")\n",
    "\n",
    "        # Mean Absolute Error (MAE)\n",
    "        mae_evaluator = RegressionEvaluator(labelCol=\"ArrDelay\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "        metrics['mae'] = mae_evaluator.evaluate(predictions)\n",
    "        print(f\"{name} - Mean Absolute Error (MAE) on test data: {metrics['mae']}\")\n",
    "\n",
    "        # R-Squared (R²)\n",
    "        r2_evaluator = RegressionEvaluator(labelCol=\"ArrDelay\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "        metrics['r2'] = r2_evaluator.evaluate(predictions)\n",
    "        print(f\"{name} - R-Squared (R²) on test data: {metrics['r2']}\")\n",
    "\n",
    "        # Store model-specific metrics\n",
    "        all_metrics[name] = metrics\n",
    "\n",
    "        # Save the best model if a save path is provided\n",
    "        if model_save_path:\n",
    "            params = cv_model.bestModel.extractParamMap()\n",
    "            model = models[name]\n",
    "            model.setParams(**params)\n",
    "            print(f\"Re-Training {best_model} model with params {params}...\")\n",
    "            # Add the current model to the pipeline\n",
    "            pipeline = Pipeline(stages=[scaler, model])\n",
    "            pipeline_model = pipeline.fit(train_df)\n",
    "            path = f\"{model_save_path}/retrained_{name}\"\n",
    "            pipeline_model.write().overwrite().save(path)\n",
    "            print(f\"Selected best model retrained saved to: {path}\")\n",
    "    return all_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T19:36:15.836357Z",
     "start_time": "2025-01-10T19:05:19.084234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading schema from JSON...\n",
      "Loading train and test datasets from Parquet...\n",
      "Train and test datasets loaded successfully!\n",
      "Train dataset count: 4724323\n",
      "Test dataset count: 1181675\n",
      "Training the model...\n",
      "Training Linear_Regression model...\n",
      "Linear_Regression - Root Mean Square Error (RMSE) on test data: 10.514251056604795\n",
      "Linear_Regression - Mean Absolute Error (MAE) on test data: 7.467296493020642\n",
      "Linear_Regression - R-Squared (R²) on test data: 0.927236775791577\n",
      "Re-Training None model with params {}...\n",
      "Selected best model retrained saved to: data/models/trained_model/retrained_Linear_Regression\n",
      "Training Decision_Tree model...\n",
      "Decision_Tree - Root Mean Square Error (RMSE) on test data: 10.517368073736392\n",
      "Decision_Tree - Mean Absolute Error (MAE) on test data: 6.818047361781439\n",
      "Decision_Tree - R-Squared (R²) on test data: 0.9271936271480895\n",
      "Re-Training None model with params {}...\n",
      "Selected best model retrained saved to: data/models/trained_model/retrained_Decision_Tree\n",
      "Training Random_Forest model...\n",
      "Random_Forest - Root Mean Square Error (RMSE) on test data: 20.479793400737353\n",
      "Random_Forest - Mean Absolute Error (MAE) on test data: 11.740894532989113\n",
      "Random_Forest - R-Squared (R²) on test data: 0.7239381515306171\n",
      "Re-Training None model with params {}...\n",
      "Selected best model retrained saved to: data/models/trained_model/retrained_Random_Forest\n",
      "Training completed. Metrics: {'Linear_Regression': {'rmse': 10.514251056604795, 'mae': 7.467296493020642, 'r2': 0.927236775791577}, 'Decision_Tree': {'rmse': 10.517368073736392, 'mae': 6.818047361781439, 'r2': 0.9271936271480895}, 'Random_Forest': {'rmse': 20.479793400737353, 'mae': 11.740894532989113, 'r2': 0.7239381515306171}}\n",
      "Making predictions on the test dataset...\n",
      "Loading model: data/models/trained_model\\retrained_Decision_Tree\n",
      "Validating model...\n",
      "retrained_Decision_Tree - Root Mean Square Error (RMSE) on test data: 18.000439254080863\n",
      "retrained_Decision_Tree - Mean Absolute Error (MAE) on test data: 10.058987479026513\n",
      "retrained_Decision_Tree - R-Squared (R²) on test data: 0.7873329259028352\n",
      "Saving predictions to data/models/trained_model/predictions/retrained_Decision_Tree_pred.csv\n",
      "Predictions saved to: data/models/trained_model/predictions/\n",
      "Loading model: data/models/trained_model\\retrained_Linear_Regression\n",
      "Validating model...\n",
      "retrained_Linear_Regression - Root Mean Square Error (RMSE) on test data: 10.435164853436449\n",
      "retrained_Linear_Regression - Mean Absolute Error (MAE) on test data: 7.457783423271152\n",
      "retrained_Linear_Regression - R-Squared (R²) on test data: 0.9285285359139296\n",
      "Saving predictions to data/models/trained_model/predictions/retrained_Linear_Regression_pred.csv\n",
      "Predictions saved to: data/models/trained_model/predictions/\n",
      "Loading model: data/models/trained_model\\retrained_Random_Forest\n",
      "Validating model...\n",
      "retrained_Random_Forest - Root Mean Square Error (RMSE) on test data: 21.130149779420186\n",
      "retrained_Random_Forest - Mean Absolute Error (MAE) on test data: 12.338982145867037\n",
      "retrained_Random_Forest - R-Squared (R²) on test data: 0.7069517036370294\n",
      "Saving predictions to data/models/trained_model/predictions/retrained_Random_Forest_pred.csv\n",
      "Predictions saved to: data/models/trained_model/predictions/\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "def test_train_and_predict(processed_schema_path, processed_train_path_parquet, processed_test_path_parquet, model_path, debug=False, skip_training=False):\n",
    "    \"\"\"\n",
    "    Function to test the build_and_train_model and predict functions using train and test datasets.\n",
    "\n",
    "    Args:\n",
    "        processed_schema_path (str): Path to the schema JSON file.\n",
    "        processed_train_path_parquet (str): Path to the train.parquet file.\n",
    "        processed_test_path_parquet (str): Path to the test.parquet file.\n",
    "        model_path (str): Path to save or load the trained model.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Spark configuration\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Optimización con recursos limitados\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.executor.memory\", \"18g\") \\\n",
    "        .config(\"spark.driver.memory\", \"42g\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "        .config(\"spark.python.worker.timeout\", \"600\") \\\n",
    "        .config(\"spark.network.timeout\", \"600s\") \\\n",
    "        .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "        .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "        .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Load schema from JSON\n",
    "    print(\"Loading schema from JSON...\")\n",
    "    with open(processed_schema_path, 'r') as f:\n",
    "        schema_json = f.read()\n",
    "    schema = StructType.fromJson(json.loads(schema_json))\n",
    "\n",
    "    # Load DataFrames from Parquet\n",
    "    print(\"Loading train and test datasets from Parquet...\")\n",
    "    train_df = spark.read.schema(schema).parquet(processed_train_path_parquet)\n",
    "    test_df = spark.read.schema(schema).parquet(processed_test_path_parquet)\n",
    "\n",
    "    # Sample and repartition data\n",
    "    \n",
    "    if debug:\n",
    "        train_df = train_df.sample(fraction=0.1).repartition(1)\n",
    "    else:\n",
    "        train_df = train_df.repartition(10)\n",
    "        \n",
    "    # train_df.persist(StorageLevel.DISK_ONLY)\n",
    "    # test_df.persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "    # Confirm data loading\n",
    "    print(\"Train and test datasets loaded successfully!\")\n",
    "    print(f\"Train dataset count: {train_df.count()}\")\n",
    "    print(f\"Test dataset count: {test_df.count()}\")\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training the model...\")\n",
    "    if not skip_training:\n",
    "        metrics = build_and_train_model(train_df, model_path)\n",
    "        print(f\"Training completed. Metrics: {metrics}\")\n",
    "\n",
    "    # Predict using the trained model\n",
    "    print(\"Making predictions on the test dataset...\")\n",
    "    output_path = model_path + \"/predictions/\"\n",
    "    \n",
    "    validate(test_df, model_path, output_path)\n",
    "        \n",
    "        \n",
    "\n",
    "# Example paths\n",
    "processed_schema_path = \"data/processed/schema.json\"\n",
    "processed_train_path_parquet = \"data/processed/train.parquet\"\n",
    "processed_test_path_parquet = \"data/processed/test.parquet\"\n",
    "model_path = \"data/models/trained_model\"\n",
    "\n",
    "# Call the function\n",
    "test_train_and_predict(processed_schema_path, processed_train_path_parquet, processed_test_path_parquet, model_path, debug=False, skip_training=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T18:32:19.709388200Z",
     "start_time": "2025-01-09T20:52:51.182613Z"
    }
   },
   "outputs": [],
   "source": [
    "# spark-submit notebook.py --mode train --input path/to/train.csv --model path/to/save_model\n",
    "# spark-submit notebook.py --mode predict --input path/to/test.csv --model path/to/save_model --output path/to/predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
