{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JAVA_TOOL_OPTIONS'] = '-Djava.security.manager=allow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Picked up JAVA_TOOL_OPTIONS: -Djava.security.manager=allow\n",
      "Picked up JAVA_TOOL_OPTIONS: -Djava.security.manager=allow\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/10 19:14:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# =================== 1. Setup Spark and Import Libraries ===================\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import RandomForestRegressor, DecisionTreeRegressor, LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import argparse\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.tuning import *\n",
    "import json\n",
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql.types import IntegerType, DoubleType, FloatType, StructField, Row\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"MachineLearningProject\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = './data/2008.csv'\n",
    "plane_data_path = './data/plane-data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(spark, df_path, planes_data_path) -> DataFrame:\n",
    "    # Read csv\n",
    "    df = spark.read.csv(\n",
    "        df_path,\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    forbidden_cols = [\n",
    "        \"ArrTime\",\n",
    "        \"ActualElapsedTime\",\n",
    "        \"AirTime\",\n",
    "        \"TaxiIn\",\n",
    "        \"Diverted\",\n",
    "        \"CarrierDelay\",\n",
    "        \"WeatherDelay\",\n",
    "        \"NASDelay\",\n",
    "        \"SecurityDelay\",\n",
    "        \"LateAircraftDelay\"\n",
    "    ]\n",
    "    df = df.drop(*forbidden_cols)\n",
    "\n",
    "    df_planes = spark.read.csv(\n",
    "        planes_data_path,\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    df_planes = df_planes.withColumnRenamed(\"tailnum\", \"TailNum\")\n",
    "    df_planes = df_planes.withColumnRenamed(\"year\", \"PlaneIssueYear\")\n",
    "    df_planes = df_planes.withColumnRenamed(\"engine_type\", \"EngineType\")\n",
    "    df_planes = df_planes.withColumnRenamed(\"aircraft_type\", \"AircraftType\")\n",
    "    df_planes = df_planes.withColumnRenamed(\"model\", \"Model\")\n",
    "    df_planes = df_planes.withColumnRenamed(\"manufacturer\", \"Manufacturer\")\n",
    "\n",
    "    data = df.join(df_planes, on=\"TailNum\", how=\"inner\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_data(df):\n",
    "    quant_time_features = [\n",
    "        'DepTime',\n",
    "        'CRSDepTime',\n",
    "        'CRSArrTime'\n",
    "    ]\n",
    "\n",
    "    quantitative_features = [\n",
    "            'CRSElapsedTime',\n",
    "            'DepDelay',\n",
    "            'Distance',\n",
    "            'TaxiOut',\n",
    "            'PlaneIssueYear'\n",
    "        ]\n",
    "\n",
    "    target_column = \"ArrDelay\"\n",
    "\n",
    "    for column in quantitative_features + [target_column]:\n",
    "        df = df.withColumn(column, col(column).cast(IntegerType()))\n",
    "    df = df.dropna(subset=[target_column])\n",
    "    null_count = df.filter(col(target_column).isNull()).count()\n",
    "\n",
    "    for column in quant_time_features:  # They are strings hhmm\n",
    "        df = df.withColumn(\n",
    "            column + \"_minutes\",\n",
    "            (F.col(column).substr(1, 2).cast(\"int\") * 60 + F.col(column).substr(3, 2).cast(\"int\"))\n",
    "        )\n",
    "        quantitative_features.append(column + \"_minutes\")\n",
    "    df = df.drop(*quant_time_features)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- CRSElapsedTime: integer (nullable = true)\n",
      " |-- ArrDelay: integer (nullable = true)\n",
      " |-- DepDelay: integer (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- TaxiOut: integer (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- Manufacturer: string (nullable = true)\n",
      " |-- issue_date: string (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- AircraftType: string (nullable = true)\n",
      " |-- EngineType: string (nullable = true)\n",
      " |-- PlaneIssueYear: integer (nullable = true)\n",
      " |-- DepTime_minutes: integer (nullable = true)\n",
      " |-- CRSDepTime_minutes: integer (nullable = true)\n",
      " |-- CRSArrTime_minutes: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = load_csv(spark, csv_path, plane_data_path)\n",
    "df = organize_data(df)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\n",
    "    'Month',\n",
    "    'DayofMonth',\n",
    "    'DayOfWeek',\n",
    "    'Year',\n",
    "    'PlaneIssueYear',\n",
    "    'DepTime_minutes',\n",
    "    'CRSDepTime_minutes',\n",
    "    'CRSArrTime_minutes',\n",
    "    'CRSElapsedTime',\n",
    "    'DepDelay',\n",
    "    'Distance',\n",
    "    'TaxiOut'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    "    'UniqueCarrier',\n",
    "    'FlightNum',\n",
    "    'TailNum',\n",
    "    'Origin',\n",
    "    'Dest',\n",
    "    'Cancelled',\n",
    "    'CancellationCode',\n",
    "    'EngineType',\n",
    "    'AircraftType',\n",
    "    'Manufacturer',\n",
    "    'Model',\n",
    "    \"issue_date\", \"status\",\n",
    "    \"type\",\n",
    "    \"ArrDelay\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 2235032\n",
      "Number of columns: 27\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rows: {df.count()}\")\n",
    "print(f\"Number of columns: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_values(data, features_list):\n",
    "    # Calculate null values for each column in features_list\n",
    "    null_data = data.select([\n",
    "        count(when(col(c).isNull() | isnan(col(c)), c)).alias(c) for c in features_list\n",
    "    ])\n",
    "    \n",
    "    # Show the results\n",
    "    null_data.show()\n",
    "    \n",
    "    # Return the DataFrame with null counts\n",
    "    return null_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_null_percentages(df, null_counts, numeric):# Convert to Pandas and compute percentage\n",
    "    if numeric == True:\n",
    "        type = 'Numerical'\n",
    "        color = 'skyblue'\n",
    "    else:\n",
    "        type = 'Categorical'\n",
    "        color = 'lightcoral'\n",
    "    total_rows = df.count()\n",
    "    null_counts_pandas = null_counts.toPandas().T  # Transpose for easier handling\n",
    "    null_counts_pandas.columns = [\"NullCount\"]\n",
    "    null_counts_pandas[\"Percentage\"] = (null_counts_pandas[\"NullCount\"] / total_rows) * 100\n",
    "    null_counts_pandas = null_counts_pandas.sort_values(\"Percentage\", ascending=False)\n",
    "\n",
    "    # Plot the bar chart\n",
    "    null_counts_pandas[\"Percentage\"].plot(kind=\"barh\", color=color)  # Use `barh` for horizontal bars\n",
    "    plt.xlabel(\"Percentage of Null Values (%)\")\n",
    "    plt.ylabel(f\"{type} Features\")\n",
    "    plt.title(f\"Percentage of Null Values by {type} Features\")\n",
    "    plt.savefig(f\"output/{type.lower()}/img/null_values_percentage_{type.lower()}.png\", dpi=300, bbox_inches=\"tight\")  # Save as PNG\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:=====>                                                   (1 + 9) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------+----+--------------+---------------+------------------+------------------+--------------+--------+--------+-------+\n",
      "|Month|DayofMonth|DayOfWeek|Year|PlaneIssueYear|DepTime_minutes|CRSDepTime_minutes|CRSArrTime_minutes|CRSElapsedTime|DepDelay|Distance|TaxiOut|\n",
      "+-----+----------+---------+----+--------------+---------------+------------------+------------------+--------------+--------+--------+-------+\n",
      "|    0|         0|        0|   0|        176935|           6950|              3077|             19365|             0|       0|       0|      0|\n",
      "+-----+----------+---------+----+--------------+---------------+------------------+------------------+--------------+--------+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "null_counts_numeric = null_values(df, numeric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "plot_null_percentages(df, null_counts_numeric, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:============================>                            (5 + 5) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-------+------+----+---------+----------------+----------+------------+------------+------+----------+------+------+--------+\n",
      "|UniqueCarrier|FlightNum|TailNum|Origin|Dest|Cancelled|CancellationCode|EngineType|AircraftType|Manufacturer| Model|issue_date|status|  type|ArrDelay|\n",
      "+-------------+---------+-------+------+----+---------+----------------+----------+------------+------------+------+----------+------+------+--------+\n",
      "|            0|        0|      0|     0|   0|        0|         2235032|    107854|      107854|      107854|107854|    107854|107854|107854|       0|\n",
      "+-------------+---------+-------+------+----+---------+----------------+----------+------------+------------+------+----------+------+------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "null_counts_categorical = null_values(df, categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/10 19:15:11 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "plot_null_percentages(df, null_counts_categorical, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Statistics Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics_summary(data):\n",
    "    summary_df = data.select(numeric_features).summary().toPandas()\n",
    "    summary_df.set_index(\"summary\", inplace=True)\n",
    "    summary_numeric = summary_df.apply(pd.to_numeric, errors='coerce')\n",
    "    summary_numeric = summary_numeric.T\n",
    "    return summary_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/10 19:15:13 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>summary</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>stddev</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Month</th>\n",
       "      <td>2235032.0</td>\n",
       "      <td>2.511060</td>\n",
       "      <td>1.123274e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DayofMonth</th>\n",
       "      <td>2235032.0</td>\n",
       "      <td>15.695071</td>\n",
       "      <td>8.745910e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DayOfWeek</th>\n",
       "      <td>2235032.0</td>\n",
       "      <td>3.915981</td>\n",
       "      <td>1.982679e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>2235032.0</td>\n",
       "      <td>2008.000000</td>\n",
       "      <td>1.908099e-13</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>2008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PlaneIssueYear</th>\n",
       "      <td>2058097.0</td>\n",
       "      <td>1995.048672</td>\n",
       "      <td>6.806124e+01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1992.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>2008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DepTime_minutes</th>\n",
       "      <td>2228082.0</td>\n",
       "      <td>1947.945860</td>\n",
       "      <td>1.662226e+03</td>\n",
       "      <td>600.0</td>\n",
       "      <td>839.0</td>\n",
       "      <td>1081.0</td>\n",
       "      <td>3604.0</td>\n",
       "      <td>5709.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRSDepTime_minutes</th>\n",
       "      <td>2231955.0</td>\n",
       "      <td>1966.182544</td>\n",
       "      <td>1.685970e+03</td>\n",
       "      <td>600.0</td>\n",
       "      <td>830.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>3660.0</td>\n",
       "      <td>5709.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRSArrTime_minutes</th>\n",
       "      <td>2215667.0</td>\n",
       "      <td>1614.865577</td>\n",
       "      <td>1.467061e+03</td>\n",
       "      <td>600.0</td>\n",
       "      <td>835.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1310.0</td>\n",
       "      <td>5709.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRSElapsedTime</th>\n",
       "      <td>2235032.0</td>\n",
       "      <td>130.778137</td>\n",
       "      <td>7.055441e+01</td>\n",
       "      <td>-21.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>660.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DepDelay</th>\n",
       "      <td>2235032.0</td>\n",
       "      <td>11.369590</td>\n",
       "      <td>3.627695e+01</td>\n",
       "      <td>-92.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2467.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Distance</th>\n",
       "      <td>2235032.0</td>\n",
       "      <td>740.248358</td>\n",
       "      <td>5.652384e+02</td>\n",
       "      <td>24.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>590.0</td>\n",
       "      <td>983.0</td>\n",
       "      <td>4962.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TaxiOut</th>\n",
       "      <td>2235032.0</td>\n",
       "      <td>16.579896</td>\n",
       "      <td>1.100051e+01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>383.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "summary                 count         mean        stddev     min     25%  \\\n",
       "Month               2235032.0     2.511060  1.123274e+00     1.0     1.0   \n",
       "DayofMonth          2235032.0    15.695071  8.745910e+00     1.0     8.0   \n",
       "DayOfWeek           2235032.0     3.915981  1.982679e+00     1.0     2.0   \n",
       "Year                2235032.0  2008.000000  1.908099e-13  2008.0  2008.0   \n",
       "PlaneIssueYear      2058097.0  1995.048672  6.806124e+01     0.0  1992.0   \n",
       "DepTime_minutes     2228082.0  1947.945860  1.662226e+03   600.0   839.0   \n",
       "CRSDepTime_minutes  2231955.0  1966.182544  1.685970e+03   600.0   830.0   \n",
       "CRSArrTime_minutes  2215667.0  1614.865577  1.467061e+03   600.0   835.0   \n",
       "CRSElapsedTime      2235032.0   130.778137  7.055441e+01   -21.0    80.0   \n",
       "DepDelay            2235032.0    11.369590  3.627695e+01   -92.0    -4.0   \n",
       "Distance            2235032.0   740.248358  5.652384e+02    24.0   329.0   \n",
       "TaxiOut             2235032.0    16.579896  1.100051e+01     0.0    10.0   \n",
       "\n",
       "summary                50%     75%     max  \n",
       "Month                  3.0     4.0     4.0  \n",
       "DayofMonth            16.0    23.0    31.0  \n",
       "DayOfWeek              4.0     6.0     7.0  \n",
       "Year                2008.0  2008.0  2008.0  \n",
       "PlaneIssueYear      2000.0  2003.0  2008.0  \n",
       "DepTime_minutes     1081.0  3604.0  5709.0  \n",
       "CRSDepTime_minutes  1070.0  3660.0  5709.0  \n",
       "CRSArrTime_minutes  1070.0  1310.0  5709.0  \n",
       "CRSElapsedTime       113.0   162.0   660.0  \n",
       "DepDelay               0.0    11.0  2467.0  \n",
       "Distance             590.0   983.0  4962.0  \n",
       "TaxiOut               14.0    19.0   383.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = statistics_summary(df)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Features Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_distributions(data, features_list, is_numeric_features=True):\n",
    "    if is_numeric_features == True:\n",
    "        type = 'Numerical'\n",
    "        numerical_df = data.select(features_list).toPandas()\n",
    "\n",
    "        num_features = len(features_list)\n",
    "        cols = 2  # Number of columns in the grid\n",
    "        rows = (num_features // cols) + (num_features % cols > 0)  # Calculate rows needed\n",
    "\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(15, rows * 5))  # Adjust the figure size\n",
    "        axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
    "\n",
    "        # Plot each feature's distribution\n",
    "        for i, col in enumerate(features_list):\n",
    "            sns.histplot(numerical_df[col], bins=30, ax=axes[i])  # Use the subplot's axis\n",
    "            axes[i].set_title(f\"Distribution of {col}\")\n",
    "            axes[i].set_xlabel(col)\n",
    "            axes[i].set_ylabel(\"Frequency\")\n",
    "\n",
    "            if col == \"DepDelay\":  # Modify based on feature name\n",
    "                axes[i].set_xlim(0, 500)  # Set x-axis range (e.g., 0 to 500)\n",
    "            elif col == \"TaxiOut\":\n",
    "                axes[i].set_xlim(0, 150)\n",
    "\n",
    "        # Remove any unused subplots\n",
    "        for i in range(len(features_list), len(axes)):\n",
    "            fig.delaxes(axes[i])\n",
    "\n",
    "        plt.tight_layout()  # Adjust layout to avoid overlap\n",
    "        plt.savefig(f\"output/{type.lower()}/img/features_distribution_{type.lower()}.png\", dpi=300, bbox_inches=\"tight\")  # Save as PNG\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        type = 'Categorical'\n",
    "        num_features = len(features_list)\n",
    "        cols = 2  # Number of columns in the grid\n",
    "        rows = (num_features // cols) + (num_features % cols > 0)  # Calculate rows needed\n",
    "\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(15, rows * 5))  # Adjust figure size\n",
    "        axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
    "\n",
    "        # Plot each feature's distribution\n",
    "        for i, col in enumerate(features_list):\n",
    "            # Group by column and count occurrences\n",
    "            # Limit to top 15 categories\n",
    "            if col == \"ArrDelay\":  # Special case for \"ArrDelay\"\n",
    "                # Group by column and count occurrences for all data\n",
    "                category_counts = data.groupBy(col).count().orderBy(\"count\", ascending=False)\n",
    "            else:\n",
    "                # Limit to top 15 categories for other features\n",
    "                top_n = 15\n",
    "                category_counts = data.groupBy(col).count().orderBy(\"count\", ascending=False).limit(top_n)\n",
    "\n",
    "            category_df = category_counts.toPandas()\n",
    "\n",
    "            # Plot using the subplot axis\n",
    "            sns.barplot(data=category_df, x=col, y=\"count\", ax=axes[i])\n",
    "            axes[i].set_title(f\"Distribution of {col}\")\n",
    "            axes[i].set_xlabel(col)\n",
    "            axes[i].set_ylabel(\"Count\")\n",
    "            axes[i].tick_params(axis=\"x\", rotation=90)  # Rotate x-axis labels\n",
    "\n",
    "            if col == \"ArrDelay\":\n",
    "                axes[i].set_xlim(50, 120)\n",
    "                axes[i].tick_params(axis=\"x\", labelsize=8)\n",
    "\n",
    "        # Remove any unused subplots\n",
    "        for i in range(len(features_list), len(axes)):\n",
    "            fig.delaxes(axes[i])\n",
    "\n",
    "        plt.tight_layout()  # Adjust layout to avoid overlap\n",
    "        plt.savefig(f\"output/{type.lower()}/img/features_distribution_{type.lower()}.png\", dpi=300, bbox_inches=\"tight\")  # Save as PNG\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "features_distributions(df, ['DayofMonth','DayOfWeek','CRSDepTime_minutes','CRSArrTime_minutes','CRSElapsedTime','DepDelay','Distance','TaxiOut'], is_numeric_features=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "features_distributions(df, ['UniqueCarrier','Origin','Dest','EngineType','AircraftType','Manufacturer','Model','ArrDelay'], is_numeric_features=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Features Proportions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportions(data, features_list, is_numeric_features = True):\n",
    "    if is_numeric_features == True:\n",
    "        type = 'numerical'\n",
    "    else:\n",
    "        type = 'categorical'\n",
    "\n",
    "    total_count = data.count()\n",
    "    for feature in features_list:\n",
    "        feature_counts = df.groupBy(feature).count()\n",
    "        # Calculate proportions\n",
    "        feature_proportions = feature_counts.withColumn(\n",
    "            \"Proportion\", round((col(\"count\") / total_count)*100,2)\n",
    "        )\n",
    "        if not os.path.exists(f'output/{type}'):\n",
    "            os.makedirs(f'output/{type}')\n",
    "        feature_proportions.write.csv(f\"output/{type}/{feature}_proportions.csv\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "proportions(df, categorical_features, is_numeric_features=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "proportions(df, numeric_features, is_numeric_features=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Average ArrDelay by categorical features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_ArrDelay(data, features_list):\n",
    "    # Number of features\n",
    "    num_features = len(features_list)\n",
    "    cols = 2  # Number of columns in the grid\n",
    "    rows = (num_features // cols) + (num_features % cols > 0)  # Calculate rows needed\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, rows * 6))  # Adjust figure size\n",
    "    axes = axes.flatten()  # Flatten axes for easy iteration\n",
    "\n",
    "    # Iterate over each categorical feature\n",
    "    for i, col_name in enumerate(features_list):  # Use col_name for clarity\n",
    "        if col_name in [\"Origin\", \"Dest\", \"Model\"]:  # Special case for \"Origin\" and \"Dest\"\n",
    "            type = 'categorical'\n",
    "            top_n = 20\n",
    "            category_counts = data.groupBy(col_name).count().orderBy(\"count\", ascending=False).limit(top_n)\n",
    "            # Filter data for the top 20 categories\n",
    "            top_categories = [row[col_name] for row in category_counts.collect()]\n",
    "            data_filtered = data.filter(col(col_name).isin(top_categories))  # Correct usage of col()\n",
    "        else:\n",
    "            type = 'numerical'\n",
    "            data_filtered = data\n",
    "\n",
    "        # Group data and calculate average arrival delay\n",
    "        grouped_df = data_filtered.groupBy(col_name).agg({\"ArrDelay\": \"mean\"})\n",
    "\n",
    "        # Convert to Pandas\n",
    "        grouped_pandas = grouped_df.toPandas()\n",
    "\n",
    "        # Plot bar chart in the subplot\n",
    "        sns.barplot(data=grouped_pandas, x=col_name, y=\"avg(ArrDelay)\", ax=axes[i])\n",
    "        axes[i].set_title(f\"Average Arrival Delay by {col_name}\", fontsize=12)\n",
    "        axes[i].set_xlabel(col_name)\n",
    "        axes[i].set_ylabel(\"Avg. Arrival Delay\")\n",
    "        axes[i].tick_params(axis=\"x\", rotation=90)\n",
    "\n",
    "        if col_name in [\"DepTime_minutes\", \"DepDelay\"]:\n",
    "            xticks = axes[i].get_xticks()\n",
    "            axes[i].set_xticks([xticks[0], xticks[-1]])\n",
    "            axes[i].set_xticklabels([grouped_pandas[col_name].iloc[0], grouped_pandas[col_name].iloc[-1]])  # Set corresponding labels\n",
    "\n",
    "\n",
    "    # Remove any unused subplots\n",
    "    for i in range(len(features_list), len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"output/{type.lower()}/img/avg_ArrDelay_{type.lower()}.png\", dpi=300, bbox_inches=\"tight\")  # Save as PNG\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "avg_ArrDelay(df, ['Origin','Dest','EngineType','AircraftType','Manufacturer','Model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "avg_ArrDelay(df, ['Month','DayofMonth','DayOfWeek','PlaneIssueYear','DepTime_minutes','DepDelay'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Correlation Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_matrix(data, features_list):\n",
    "    data = data.fillna(0, subset=features_list)\n",
    "    vector_col = \"features_corr\"\n",
    "\n",
    "    vector_assembler = VectorAssembler(inputCols=features_list, outputCol=vector_col)\n",
    "    df_vector = vector_assembler.transform(data)\n",
    "\n",
    "    # Compute Correlation Matrix\n",
    "    correlation_matrix = Correlation.corr(df_vector, vector_col).head()[0]  # Get the DenseMatrix\n",
    "    correlation_array = correlation_matrix.toArray()\n",
    "    correlation_df = pd.DataFrame(correlation_array, index=features_list, columns=features_list)\n",
    "    sns.heatmap(\n",
    "        correlation_df,\n",
    "        annot=True,              # Show the correlation values\n",
    "        fmt=\".1f\",               # Format to two decimal places\n",
    "        cmap=\"coolwarm\",         # Color map\n",
    "        annot_kws={\"size\": 8}    # Reduce annotation font size\n",
    "    )\n",
    "    plt.title(\"Correlation Matrix Heatmap\")\n",
    "    plt.savefig(f\"output/numerical/img/correlation_matrix.png\", dpi=300, bbox_inches=\"tight\")  # Save as PNG\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/10 19:16:24 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/01/10 19:16:24 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "25/01/10 19:16:25 WARN PearsonCorrelation: Pearson correlation matrix contains NaN values.\n"
     ]
    }
   ],
   "source": [
    "corr_matrix(df, numeric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== 3. Data Processing ===================\n",
    "def process_data(data, mode):\n",
    "    \"\"\"\n",
    "    Process the dataset: handle missing values and perform feature engineering.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): Spark DataFrame to process.\n",
    "        mode (str): Mode of operation (\"train\" or \"predict\").\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Processed Spark DataFrame with new features added.\n",
    "    \"\"\"\n",
    "    # Validate the target variable for training mode\n",
    "    if mode == \"train\" and \"ArrDelay\" not in data.columns:\n",
    "        raise ValueError(\"The target variable 'ArrDelay' is missing.\")\n",
    "\n",
    "    # Handle missing values\n",
    "    if mode == \"train\":\n",
    "        # Drop rows where the target variable or features are null\n",
    "        data = data.dropna(subset=[\"ArrDelay\"])\n",
    "\n",
    "    # Example: Fill null values in specific columns with a default value\n",
    "    # Replace 'column_name' with actual column names as needed\n",
    "    # Uncomment this if specific columns require filling\n",
    "    # data = data.fillna({\"column_name\": 0})\n",
    "\n",
    "    # Transform special variables\n",
    "    # Feature engineering: Create time-based features\n",
    "    if \"DepTime\" in data.columns:\n",
    "        data = data.withColumn(\"DepHour\", (col(\"DepTime\") / 100).cast(\"int\"))  # Extract hour from departure time\n",
    "\n",
    "    if \"FlightDate\" in data.columns:\n",
    "        data = data.withColumn(\"DayOfWeek\", date_format(col(\"FlightDate\"), \"u\").cast(\"int\"))  # Convert to day of the week\n",
    "\n",
    "    # Feature engineering: Create flight distance categories\n",
    "    if \"Distance\" in data.columns:\n",
    "        data = data.withColumn(\n",
    "            \"DistanceCategory\",\n",
    "            when(col(\"Distance\") < 500, \"Short\")  # Short flights\n",
    "            .when((col(\"Distance\") >= 500) & (col(\"Distance\") < 1500), \"Medium\")  # Medium flights\n",
    "            .otherwise(\"Long\")  # Long flights\n",
    "        )\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== 4. Feature Engineering ===================\n",
    "def feature_engineering(data, additional_dataset_path=None):\n",
    "    \"\"\"\n",
    "    Perform feature engineering, including creating new features and optionally integrating additional datasets.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): Spark DataFrame for feature engineering.\n",
    "        additional_dataset_path (str): Optional path to an additional dataset for integration.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Enhanced Spark DataFrame with new features.\n",
    "    \"\"\"\n",
    "    # Create new features based on existing columns\n",
    "    if \"existing_column\" in data.columns:\n",
    "        data = data.withColumn(\"new_feature\", col(\"existing_column\") * 2)  # Example transformation\n",
    "\n",
    "    # Optional: Integrate additional datasets\n",
    "    if additional_dataset_path:\n",
    "        try:\n",
    "            additional_dataset = spark.read.csv(additional_dataset_path, header=True, inferSchema=True)\n",
    "            \n",
    "            # Example: Join the datasets on a common column\n",
    "            if \"common_column\" in data.columns and \"common_column\" in additional_dataset.columns:\n",
    "                data = data.join(additional_dataset, on=\"common_column\", how=\"left\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error integrating additional dataset: {e}\")\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train_model(data, pipeline, model_save_path=None):\n",
    "    \"\"\"\n",
    "    Build, train, evaluate, and optionally save the model using cross-validation with three models.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): Spark DataFrame with features and labels.\n",
    "        pipeline (Pipeline): Preprocessing pipeline to use before modeling.\n",
    "        model_save_path (str): Path to save the trained model (optional).\n",
    "    \n",
    "    Returns:\n",
    "        dict: Evaluation metrics for the trained model.\n",
    "    \"\"\"\n",
    "    # Split data into training and testing sets\n",
    "    train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "    # Define the models: RandomForestRegressor, DecisionTreeRegressor, LinearRegression\n",
    "    rf = RandomForestRegressor(featuresCol=\"features_vector\", labelCol=\"ArrDelay\")\n",
    "    dt = DecisionTreeRegressor(featuresCol=\"features_vector\", labelCol=\"ArrDelay\")\n",
    "    lr = LinearRegression(featuresCol=\"features_vector\", labelCol=\"ArrDelay\")\n",
    "\n",
    "    # Initialize metrics dictionary\n",
    "    all_metrics = {}\n",
    "\n",
    "    # Evaluate each model separately\n",
    "    models = [rf, dt, lr]\n",
    "    model_names = ['Random Forest', 'Decision Tree', 'Linear Regression']\n",
    "    \n",
    "    for model, name in zip(models, model_names):\n",
    "        print(f\"Training {name} model...\")\n",
    "        \n",
    "        # Add the current model to the pipeline\n",
    "        pipeline.setStages(pipeline.getStages() + [model])\n",
    "\n",
    "        # Hyperparameter tuning with cross-validation for the current model\n",
    "        param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(model.numTrees, [10, 50, 100]) if isinstance(model, RandomForestRegressor) else \\\n",
    "            (addGrid(model.maxDepth, [5, 10, 20]) if isinstance(model, DecisionTreeRegressor) else \\\n",
    "            addGrid(model.regParam, [0.1, 0.3, 0.5])) \\\n",
    "            .build()\n",
    "\n",
    "        evaluator = RegressionEvaluator(labelCol=\"ArrDelay\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "        # Set up cross-validation\n",
    "        cv = CrossValidator(\n",
    "            estimator=pipeline,\n",
    "            estimatorParamMaps=param_grid,\n",
    "            evaluator=evaluator,\n",
    "            numFolds=5\n",
    "        )\n",
    "\n",
    "        # Train the model with cross-validation\n",
    "        cv_model = cv.fit(train_data)\n",
    "\n",
    "        # Generate predictions on the test dataset\n",
    "        predictions = cv_model.transform(test_data)\n",
    "\n",
    "        # Evaluate the model using multiple metrics\n",
    "        metrics = {}\n",
    "        # Root Mean Square Error (RMSE)\n",
    "        rmse_evaluator = RegressionEvaluator(labelCol=\"ArrDelay\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "        metrics['rmse'] = rmse_evaluator.evaluate(predictions)\n",
    "        print(f\"{name} - Root Mean Square Error (RMSE) on test data: {metrics['rmse']}\")\n",
    "\n",
    "        # Mean Absolute Error (MAE)\n",
    "        mae_evaluator = RegressionEvaluator(labelCol=\"ArrDelay\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "        metrics['mae'] = mae_evaluator.evaluate(predictions)\n",
    "        print(f\"{name} - Mean Absolute Error (MAE) on test data: {metrics['mae']}\")\n",
    "\n",
    "        # R-Squared (R²)\n",
    "        r2_evaluator = RegressionEvaluator(labelCol=\"ArrDelay\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "        metrics['r2'] = r2_evaluator.evaluate(predictions)\n",
    "        print(f\"{name} - R-Squared (R²) on test data: {metrics['r2']}\")\n",
    "\n",
    "        # Store model-specific metrics\n",
    "        all_metrics[name] = metrics\n",
    "\n",
    "        # Save the best model if a save path is provided\n",
    "        if model_save_path:\n",
    "            cv_model.bestModel.write().overwrite().save(f\"{model_save_path}_{name}\")\n",
    "            print(f\"Best {name} model saved to: {model_save_path}_{name}\")\n",
    "\n",
    "    return all_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, model_path, output_path):\n",
    "    \"\"\"\n",
    "    Load the trained model and generate predictions for the given data.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): Spark DataFrame for predictions.\n",
    "        model_path (str): Path to load the trained model.\n",
    "        output_path (str): Path to save predictions (CSV).\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load the trained model\n",
    "    model = PipelineModel.load(model_path)\n",
    "\n",
    "    # Make predictions on the input data\n",
    "    predictions = model.transform(data)\n",
    "\n",
    "    # Save predictions to the specified output path\n",
    "    predictions.select(\"features_vector\", \"prediction\").write.csv(output_path, header=True)\n",
    "    print(f\"Predictions saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the pipeline workflow.\n",
    "    Accepts command-line arguments for dynamic input/output handling.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Flight Delay Prediction Application\")\n",
    "    parser.add_argument(\"--mode\", type=str, required=True, choices=[\"train\", \"predict\"], help=\"Mode: train or predict\")\n",
    "    parser.add_argument(\"--input\", type=str, required=True, help=\"Path to input CSV file\")\n",
    "    parser.add_argument(\"--model\", type=str, required=True, help=\"Path to save/load the model\")\n",
    "    parser.add_argument(\"--output\", type=str, help=\"Path to save predictions (required for predict mode)\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Start Spark Session\n",
    "    spark = SparkSession.builder.appName(\"FlightDelayPipeline\").getOrCreate()\n",
    "\n",
    "    try:\n",
    "        # Workflow\n",
    "        data = load_data(spark, args.input, args.mode)  # Load the dataset\n",
    "        data = eda(data)\n",
    "        data = process_data(data, args.mode)        # Preprocess the dataset\n",
    "        pipeline, _ = feature_engineering(data)        # Perform feature engineering\n",
    "\n",
    "        if args.mode == \"train\":\n",
    "            # Train the model, evaluate it, and optionally save it\n",
    "            metrics = build_and_train_model(data, pipeline, args.model)\n",
    "            print(f\"Training completed. Evaluation metrics: {metrics}\")\n",
    "        elif args.mode == \"predict\":\n",
    "            if not args.output:\n",
    "                raise ValueError(\"Output path is required for prediction mode.\")\n",
    "            # Use the trained model to generate predictions\n",
    "            predict(data, args.model, args.output)\n",
    "\n",
    "    finally:\n",
    "        # Stop Spark Session\n",
    "        spark.stop()\n",
    "\n",
    "# Add this block to execute the script when running it as a standalone script\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark-submit notebook.py --mode train --input path/to/train.csv --model path/to/save_model\n",
    "# spark-submit notebook.py --mode predict --input path/to/test.csv --model path/to/save_model --output path/to/predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
