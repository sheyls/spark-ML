{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/01 22:17:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/01/01 22:17:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# =================== 1. Setup Spark and Import Libraries ===================\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import RandomForestRegressor, DecisionTreeRegressor, LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import argparse\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.tuning import *\n",
    "\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"MachineLearningProject\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descomprimiendo archivos...\n",
      "Advertencia: 1991.csv no encontrado en 1991.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 1991.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 1991.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 1991.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 1991.\n",
      "Advertencia: 2004.csv no encontrado en 2004.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 2004.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 2004.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 2004.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 2004.\n",
      "Advertencia: 1988.csv no encontrado en 1988.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 1988.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 1988.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 1988.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 1988.\n",
      "Advertencia: 1998.csv no encontrado en 1998.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 1998.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 1998.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 1998.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 1998.\n",
      "Advertencia: 1999.csv no encontrado en 1999.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 1999.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 1999.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 1999.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 1999.\n",
      "Advertencia: 1989.csv no encontrado en 1989.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 1989.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 1989.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 1989.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 1989.\n",
      "Advertencia: 1990.csv no encontrado en 1990.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 1990.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 1990.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 1990.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 1990.\n",
      "Advertencia: 2005.csv no encontrado en 2005.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 2005.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 2005.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 2005.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 2005.\n",
      "Advertencia: 2007.csv no encontrado en 2007.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 2007.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 2007.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 2007.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 2007.\n",
      "Advertencia: 1992.csv no encontrado en 1992.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 1992.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 1992.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 1992.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 1992.\n",
      "Advertencia: 2006.csv no encontrado en 2006.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 2006.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 2006.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 2006.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 2006.\n",
      "Advertencia: 1993.csv no encontrado en 1993.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 1993.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 1993.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 1993.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 1993.\n",
      "Advertencia: 1996.csv no encontrado en 1996.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 1996.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 1996.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 1996.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 1996.\n",
      "Advertencia: 2003.csv no encontrado en 2003.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 2003.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 2003.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 2003.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 2003.\n",
      "Advertencia: 1987.csv no encontrado en 1987.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 1987.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 1987.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 1987.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 1987.\n",
      "Advertencia: 1997.csv no encontrado en 1997.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 1997.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 1997.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 1997.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 1997.\n",
      "Advertencia: 2002.csv no encontrado en 2002.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 2002.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 2002.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 2002.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 2002.\n",
      "Advertencia: 2000.csv no encontrado en 2000.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 2000.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 2000.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 2000.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 2000.\n",
      "Advertencia: 1995.csv no encontrado en 1995.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 1995.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 1995.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 1995.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 1995.\n",
      "Advertencia: 2008.csv no encontrado en 2008.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 2008.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 2008.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 2008.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 2008.\n",
      "Advertencia: 2001.csv no encontrado en 2001.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 2001.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 2001.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 2001.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 2001.\n",
      "Advertencia: 1994.csv no encontrado en 1994.csv.bz2\n",
      "Advertencia: plane-data.csv no encontrado en 1994.csv.bz2\n",
      "Advertencia: carriers.csv no encontrado en 1994.csv.bz2\n",
      "Advertencia: airports.csv no encontrado en 1994.csv.bz2\n",
      "Archivos descomprimidos correctamente para el año 1994.\n",
      "Procesando todos los datos anuales...\n",
      "Procesando datos del año: 1988\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/Users/sheyls/study/big_data/spark-ML/data/new/1988/1988.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 100\u001b[0m\n\u001b[1;32m     97\u001b[0m combined_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m year_folder \u001b[38;5;129;01min\u001b[39;00m year_folders:\n\u001b[0;32m--> 100\u001b[0m     yearly_data \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_yearly_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m combined_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m         combined_data \u001b[38;5;241m=\u001b[39m yearly_data\n",
      "Cell \u001b[0;32mIn[19], line 69\u001b[0m, in \u001b[0;36mprocess_yearly_data\u001b[0;34m(year_folder)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcesando datos del año: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Leer los archivos\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m flights \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43myear\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m plane_data \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(year_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplane-data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m), header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     71\u001b[0m carriers \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(year_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcarriers.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m), header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/Users/sheyls/study/big_data/spark-ML/data/new/1988/1988.csv."
     ]
    }
   ],
   "source": [
    "# =================== 1. Data Reading ===================\n",
    "def load_data(spark, input_path, mode):\n",
    "    \"\"\"\n",
    "    Load the input dataset, drop forbidden columns, and validate its structure.\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): The active Spark session.\n",
    "        input_path (str): Path to the input CSV file.\n",
    "        mode (str): Mode of operation (\"train\" or \"predict\").\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Processed Spark DataFrame.\n",
    "    \"\"\"\n",
    "    forbidden_columns = [\n",
    "        \"ArrTime\", \"ActualElapsedTime\", \"AirTime\", \"TaxiIn\", \"Diverted\",\n",
    "        \"CarrierDelay\", \"WeatherDelay\", \"NASDelay\", \"SecurityDelay\", \"LateAircraftDelay\"\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # Read the dataset\n",
    "        data = spark.read.csv(input_path, header=True, inferSchema=True)\n",
    "\n",
    "        # Drop forbidden columns\n",
    "        data = data.drop(*forbidden_columns)\n",
    "\n",
    "        # Check if the dataset is empty\n",
    "        if data.count() == 0:\n",
    "            raise ValueError(\"The dataset is empty.\")\n",
    "\n",
    "        # Validate the presence of the target variable for training\n",
    "        if mode == \"train\" and \"ArrDelay\" not in data.columns:\n",
    "            raise ValueError(\"The target variable 'ArrDelay' is missing.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the dataset: {e}\")\n",
    "        spark.stop()\n",
    "        raise\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== 2. Exploratory Data Analysis (EDA) ===================\n",
    "def eda(data):\n",
    "    \"\"\"\n",
    "    Perform exploratory data analysis on the dataset, including univariate and multivariate analysis.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): Spark DataFrame to analyze.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with a feature vector column added for further processing.\n",
    "    \"\"\"\n",
    "    # Univariate analysis: Display statistical summary\n",
    "    print(\"Statistical Summary:\")\n",
    "    data.describe().show()\n",
    "\n",
    "    # Multivariate analysis: Correlations and patterns\n",
    "    # Select numeric columns for correlation analysis\n",
    "    numeric_cols = [col for col, dtype in data.dtypes if dtype in ('int', 'double')]\n",
    "\n",
    "    if numeric_cols:\n",
    "        # Assemble numeric columns into a single feature vector\n",
    "        vector_col = \"features_vector\"\n",
    "        assembler = VectorAssembler(inputCols=numeric_cols, outputCol=vector_col)\n",
    "        data = assembler.transform(data)\n",
    "\n",
    "        # Display correlation matrix for the numeric features\n",
    "        from pyspark.ml.stat import Correlation\n",
    "        try:\n",
    "            correlation_matrix = Correlation.corr(data, vector_col).head()[0]\n",
    "            print(f\"Correlation matrix:\\n{correlation_matrix}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating correlations: {e}\")\n",
    "    else:\n",
    "        print(\"No numeric columns available for correlation analysis.\")\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== 3. Data Processing ===================\n",
    "def process_data(data, mode):\n",
    "    \"\"\"\n",
    "    Process the dataset: handle missing values and perform feature engineering.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): Spark DataFrame to process.\n",
    "        mode (str): Mode of operation (\"train\" or \"predict\").\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Processed Spark DataFrame with new features added.\n",
    "    \"\"\"\n",
    "    # Validate the target variable for training mode\n",
    "    if mode == \"train\" and \"ArrDelay\" not in data.columns:\n",
    "        raise ValueError(\"The target variable 'ArrDelay' is missing.\")\n",
    "\n",
    "    # Handle missing values\n",
    "    if mode == \"train\":\n",
    "        # Drop rows where the target variable or features are null\n",
    "        data = data.dropna(subset=[\"ArrDelay\"])\n",
    "\n",
    "    # Example: Fill null values in specific columns with a default value\n",
    "    # Replace 'column_name' with actual column names as needed\n",
    "    # Uncomment this if specific columns require filling\n",
    "    # data = data.fillna({\"column_name\": 0})\n",
    "\n",
    "    # Transform special variables\n",
    "    # Feature engineering: Create time-based features\n",
    "    if \"DepTime\" in data.columns:\n",
    "        data = data.withColumn(\"DepHour\", (col(\"DepTime\") / 100).cast(\"int\"))  # Extract hour from departure time\n",
    "\n",
    "    if \"FlightDate\" in data.columns:\n",
    "        data = data.withColumn(\"DayOfWeek\", date_format(col(\"FlightDate\"), \"u\").cast(\"int\"))  # Convert to day of the week\n",
    "\n",
    "    # Feature engineering: Create flight distance categories\n",
    "    if \"Distance\" in data.columns:\n",
    "        data = data.withColumn(\n",
    "            \"DistanceCategory\",\n",
    "            when(col(\"Distance\") < 500, \"Short\")  # Short flights\n",
    "            .when((col(\"Distance\") >= 500) & (col(\"Distance\") < 1500), \"Medium\")  # Medium flights\n",
    "            .otherwise(\"Long\")  # Long flights\n",
    "        )\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== 4. Feature Engineering ===================\n",
    "def feature_engineering(data, additional_dataset_path=None):\n",
    "    \"\"\"\n",
    "    Perform feature engineering, including creating new features and optionally integrating additional datasets.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): Spark DataFrame for feature engineering.\n",
    "        additional_dataset_path (str): Optional path to an additional dataset for integration.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Enhanced Spark DataFrame with new features.\n",
    "    \"\"\"\n",
    "    # Create new features based on existing columns\n",
    "    if \"existing_column\" in data.columns:\n",
    "        data = data.withColumn(\"new_feature\", col(\"existing_column\") * 2)  # Example transformation\n",
    "\n",
    "    # Optional: Integrate additional datasets\n",
    "    if additional_dataset_path:\n",
    "        try:\n",
    "            additional_dataset = spark.read.csv(additional_dataset_path, header=True, inferSchema=True)\n",
    "            \n",
    "            # Example: Join the datasets on a common column\n",
    "            if \"common_column\" in data.columns and \"common_column\" in additional_dataset.columns:\n",
    "                data = data.join(additional_dataset, on=\"common_column\", how=\"left\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error integrating additional dataset: {e}\")\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train_model(data, pipeline, model_save_path=None):\n",
    "    \"\"\"\n",
    "    Build, train, evaluate, and optionally save the model using cross-validation with three models.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): Spark DataFrame with features and labels.\n",
    "        pipeline (Pipeline): Preprocessing pipeline to use before modeling.\n",
    "        model_save_path (str): Path to save the trained model (optional).\n",
    "    \n",
    "    Returns:\n",
    "        dict: Evaluation metrics for the trained model.\n",
    "    \"\"\"\n",
    "    # Split data into training and testing sets\n",
    "    train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "    # Define the models: RandomForestRegressor, DecisionTreeRegressor, LinearRegression\n",
    "    rf = RandomForestRegressor(featuresCol=\"features_vector\", labelCol=\"ArrDelay\")\n",
    "    dt = DecisionTreeRegressor(featuresCol=\"features_vector\", labelCol=\"ArrDelay\")\n",
    "    lr = LinearRegression(featuresCol=\"features_vector\", labelCol=\"ArrDelay\")\n",
    "\n",
    "    # Initialize metrics dictionary\n",
    "    all_metrics = {}\n",
    "\n",
    "    # Evaluate each model separately\n",
    "    models = [rf, dt, lr]\n",
    "    model_names = ['Random Forest', 'Decision Tree', 'Linear Regression']\n",
    "    \n",
    "    for model, name in zip(models, model_names):\n",
    "        print(f\"Training {name} model...\")\n",
    "        \n",
    "        # Add the current model to the pipeline\n",
    "        pipeline.setStages(pipeline.getStages() + [model])\n",
    "\n",
    "        # Hyperparameter tuning with cross-validation for the current model\n",
    "        param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(model.numTrees, [10, 50, 100]) if isinstance(model, RandomForestRegressor) else \\\n",
    "            (addGrid(model.maxDepth, [5, 10, 20]) if isinstance(model, DecisionTreeRegressor) else \\\n",
    "            addGrid(model.regParam, [0.1, 0.3, 0.5])) \\\n",
    "            .build()\n",
    "\n",
    "        evaluator = RegressionEvaluator(labelCol=\"ArrDelay\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "        # Set up cross-validation\n",
    "        cv = CrossValidator(\n",
    "            estimator=pipeline,\n",
    "            estimatorParamMaps=param_grid,\n",
    "            evaluator=evaluator,\n",
    "            numFolds=5\n",
    "        )\n",
    "\n",
    "        # Train the model with cross-validation\n",
    "        cv_model = cv.fit(train_data)\n",
    "\n",
    "        # Generate predictions on the test dataset\n",
    "        predictions = cv_model.transform(test_data)\n",
    "\n",
    "        # Evaluate the model using multiple metrics\n",
    "        metrics = {}\n",
    "        # Root Mean Square Error (RMSE)\n",
    "        rmse_evaluator = RegressionEvaluator(labelCol=\"ArrDelay\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "        metrics['rmse'] = rmse_evaluator.evaluate(predictions)\n",
    "        print(f\"{name} - Root Mean Square Error (RMSE) on test data: {metrics['rmse']}\")\n",
    "\n",
    "        # Mean Absolute Error (MAE)\n",
    "        mae_evaluator = RegressionEvaluator(labelCol=\"ArrDelay\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "        metrics['mae'] = mae_evaluator.evaluate(predictions)\n",
    "        print(f\"{name} - Mean Absolute Error (MAE) on test data: {metrics['mae']}\")\n",
    "\n",
    "        # R-Squared (R²)\n",
    "        r2_evaluator = RegressionEvaluator(labelCol=\"ArrDelay\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "        metrics['r2'] = r2_evaluator.evaluate(predictions)\n",
    "        print(f\"{name} - R-Squared (R²) on test data: {metrics['r2']}\")\n",
    "\n",
    "        # Store model-specific metrics\n",
    "        all_metrics[name] = metrics\n",
    "\n",
    "        # Save the best model if a save path is provided\n",
    "        if model_save_path:\n",
    "            cv_model.bestModel.write().overwrite().save(f\"{model_save_path}_{name}\")\n",
    "            print(f\"Best {name} model saved to: {model_save_path}_{name}\")\n",
    "\n",
    "    return all_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, model_path, output_path):\n",
    "    \"\"\"\n",
    "    Load the trained model and generate predictions for the given data.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): Spark DataFrame for predictions.\n",
    "        model_path (str): Path to load the trained model.\n",
    "        output_path (str): Path to save predictions (CSV).\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load the trained model\n",
    "    model = PipelineModel.load(model_path)\n",
    "\n",
    "    # Make predictions on the input data\n",
    "    predictions = model.transform(data)\n",
    "\n",
    "    # Save predictions to the specified output path\n",
    "    predictions.select(\"features_vector\", \"prediction\").write.csv(output_path, header=True)\n",
    "    print(f\"Predictions saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the pipeline workflow.\n",
    "    Accepts command-line arguments for dynamic input/output handling.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Flight Delay Prediction Application\")\n",
    "    parser.add_argument(\"--mode\", type=str, required=True, choices=[\"train\", \"predict\"], help=\"Mode: train or predict\")\n",
    "    parser.add_argument(\"--input\", type=str, required=True, help=\"Path to input CSV file\")\n",
    "    parser.add_argument(\"--model\", type=str, required=True, help=\"Path to save/load the model\")\n",
    "    parser.add_argument(\"--output\", type=str, help=\"Path to save predictions (required for predict mode)\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Start Spark Session\n",
    "    spark = SparkSession.builder.appName(\"FlightDelayPipeline\").getOrCreate()\n",
    "\n",
    "    try:\n",
    "        # Workflow\n",
    "        data = load_data(spark, args.input, args.mode)  # Load the dataset\n",
    "        data = eda(data)\n",
    "        data = process_data(data, args.mode)        # Preprocess the dataset\n",
    "        pipeline, _ = feature_engineering(data)        # Perform feature engineering\n",
    "\n",
    "        if args.mode == \"train\":\n",
    "            # Train the model, evaluate it, and optionally save it\n",
    "            metrics = build_and_train_model(data, pipeline, args.model)\n",
    "            print(f\"Training completed. Evaluation metrics: {metrics}\")\n",
    "        elif args.mode == \"predict\":\n",
    "            if not args.output:\n",
    "                raise ValueError(\"Output path is required for prediction mode.\")\n",
    "            # Use the trained model to generate predictions\n",
    "            predict(data, args.model, args.output)\n",
    "\n",
    "    finally:\n",
    "        # Stop Spark Session\n",
    "        spark.stop()\n",
    "\n",
    "# Add this block to execute the script when running it as a standalone script\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark-submit notebook.py --mode train --input path/to/train.csv --model path/to/save_model\n",
    "# spark-submit notebook.py --mode predict --input path/to/test.csv --model path/to/save_model --output path/to/predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
